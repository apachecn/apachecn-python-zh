# 并发性

在本章中，我们将介绍以下配方：

*   线程池通过线程池并发运行任务
*   协同路由通过协同路由交错执行代码
*   将工作分派到多个子流程的流程
*   期货是一项将在未来完成的任务
*   计划任务将任务设置为在给定时间或每隔几秒钟运行一次
*   在流程之间共享数据管理跨多个流程访问的变量

# 介绍

并发是指在同一时间范围内运行两个或多个任务的能力，无论它们是否并行。Python 提供了许多工具来实现并发和异步行为：线程、协同路由和进程。虽然其中一些由于其设计（协同路由）或全局解释器锁（线程）而不允许真正的并行，但它们非常易于使用，可以利用它们执行并行 I/O 操作或以最小的工作量交错函数。当需要真正的并行性时，Python 中的多处理非常容易，可以成为任何类型软件的可行解决方案。

本章将介绍在 Python 中实现并发性的最常见方法，将向您展示如何执行在后台等待特定条件的异步任务，以及如何在进程之间共享数据。

# 线程池

从历史上看，线程一直是在软件中实现并发性的最常见方式。

理论上，当系统允许时，这些线程可以实现真正的并行，但在 Python 中，**全局解释器锁**（**GLI**）不允许线程实际利用多核系统，因为该锁将允许单个 Python 操作在任何给定时间进行。

由于这个原因，在 Python 中线程常常被低估，但事实上，即使涉及 GIL，它们也可以是并发运行 I/O 操作的非常方便的解决方案。

在使用协程时，我们需要一个`run`循环和一些自定义代码来确保 I/O 操作并行进行。使用线程，我们可以在一个线程中运行任何类型的函数，如果该函数执行某种类型的 I/O，例如从套接字或磁盘读取，那么其他线程将同时继续。

线程的主要缺点之一是产生线程的成本。这经常被认为是为什么协同路由是一个更好的解决方案的原因之一，但有一种方法可以避免在需要线程时支付该成本：`ThreadPool`。

`ThreadPool`是一组线程，通常在应用程序启动时启动，在实际有一些工作要分派之前，它什么也不做。这样，当我们有一个任务想要运行到一个单独的线程中时，我们只需要将它发送到`ThreadPool`，而`ThreadPool`将把它分配给它拥有的所有线程中的第一个可用线程。由于这些线程已经存在并正在运行，我们不必在每次有工作要做时支付生成线程的成本。

# 怎么做。。。

此配方的步骤如下所示：

1.  为了展示`ThreadPool`是如何工作的，我们需要同时运行两个操作。您将从 web 获取 URL，这可能需要一些时间：

```
def fetch_url(url):
    """Fetch content of a given url from the web"""
    import urllib.request
    response = urllib.request.urlopen(url)
    return response.read()
```

2.  另一个只需等待给定条件为真，反复循环，直到完成：

```
def wait_until(predicate):
    """Waits until the given predicate returns True"""
    import time
    seconds = 0
    while not predicate():
        print('Waiting...')
        time.sleep(1.0)
        seconds += 1
    print('Done!')
    return seconds
```

3.  然后我们将启动`https://httpbin.org/delay/3`的下载，这将花费 3 秒钟，并同时等待下载完成。
4.  为此，我们将在`ThreadPool`（四个线程）中运行这两个任务，并等待它们完成：

```
>>> from multiprocessing.pool import ThreadPool
>>> pool = ThreadPool(4)
>>> t1 = pool.apply_async(fetch_url, args=('https://httpbin.org/delay/3',))
>>> t2 = pool.apply_async(wait_until, args=(t1.ready, ))
Waiting...
>>> pool.close()
>>> pool.join()
Waiting...
Waiting...
Waiting...
Done!
>>> print('Total Time:', t2.get())
Total Time: 4
>>> print('Content:', t1.get())
Content: b'{"args":{},"data":"","files":{},"form":{},
            "headers":{"Accept-Encoding":"identity",
            "Connection":"close","Host":"httpbin.org",
            "User-Agent":"Python-urllib/3.5"},
            "origin":"99.199.99.199",
            "url":"https://httpbin.org/delay/3"}\n'
```

# 它是如何工作的。。。

`ThreadPool`由两个主要组件组成：一组线程和一组队列。创建池时，将启动一些业务流程线程，并启动池初始化时指定的工作线程数。

工作线程将负责实际运行您分派给它们的任务，而业务流程线程将负责管理工作线程，执行诸如在池关闭时告诉它们退出，或在它们崩溃时重新启动等操作。

如果没有提供工作线程数量，`TaskPool`将只启动与`os.cpu_count()`返回的系统内核数量相同的线程。

一旦线程启动，它们就会坐在那里等待从包含要完成的工作的队列中消费一些东西。一旦队列有一个条目，工作线程就会唤醒并使用它，开始工作。

工作完成后，作业及其结果将被放回结果队列，以便等待它们的人可以获取它们。

因此，当我们创建`TaskPool`时，我们实际上启动了四个工人，他们开始等待任务队列中的任何事情：

```
>>> pool = ThreadPool(4)
```

然后，一旦我们为`TaskPool`提供了工作，我们实际上将两个函数排队到任务队列中，当一个工作线程可用时，它就会获取其中一个并开始运行它：

```
>>> t1 = pool.apply_async(fetch_url, args=('https://httpbin.org/delay/3',))
```

同时，`TaskPool`返回一个`AsyncResult`对象，它有两个有趣的方法：`AsyncResult.ready()`告诉我们结果是否准备好（任务完成），以及`AsyncResult.get()`，一旦结果可用，它将返回结果。

我们排队的第二个函数是等待特定谓词为`True`的函数，在本例中，我们提供了`t1.ready`，这是前面`AsyncResult`的 ready 方法：

```
>>> t2 = pool.apply_async(wait_until, args=(t1.ready, ))
```

这意味着第二个任务将在第一个任务完成后完成，因为它将等待`t1.ready() == True`。

一旦两个任务都在运行，我们会告诉`pool`我们没有更多的事情要做，这样它可以在完成它正在做的事情后退出：

```
>>> pool.close()
```

我们等待`pool`退出：

```
>>> pool.join()
```

这样，我们将等待两个任务完成，然后退出由`pool`启动的所有线程。

一旦我们知道所有任务都完成了（因为`pool.join()`返回），我们就可以抓取结果并打印出来：

```
>>> print('Total Time:', t2.get())
Total Time: 4
>>> print('Content:', t1.get())
Content: b'{"args":{},"data":"","files":{},"form":{},
            "headers":{"Accept-Encoding":"identity",
            "Connection":"close","Host":"httpbin.org",
            "User-Agent":"Python-urllib/3.5"},
            "origin":"99.199.99.199",
            "url":"https://httpbin.org/delay/3"}\n'
```

如果我们有更多的工作要做，我们将避免运行`pool.close()`和`pool.join()`方法，这样我们就可以向`TaskPool`发送更多的工作，这将在没有线程时立即完成。

# 还有更多。。。

当您有多个条目需要反复应用同一操作时，`ThreadPool`特别方便。假设您有四个需要下载的 URL 列表：

```
urls = [
    "https://httpbin.org/delay/1",
    "https://httpbin.org/delay/2",
    "https://httpbin.org/delay/3",
    "https://httpbin.org/delay/4"
]
```

在单个线程中获取它们将花费大量时间：

```
def fetch_all_urls():
    contents = []
    for url in urls:
        contents.append(fetch_url(url))
    return contents
```

我们可以通过`timeit`模块运行该功能来测试时间：

```
>>> import timeit
>>> timeit.timeit(fetch_all_urls, number=1)
12.116707602981478
```

如果我们可以为每个函数使用一个单独的线程，那么只需要最慢的线程就可以获取所有提供的 URL，因为下载将同时进行。

`ThreadPool`实际上为我们提供了`map`方法，该方法正好做到了这一点：它将函数应用于参数列表：

```
def fetch_all_urls_theraded():
    pool = ThreadPool(4)
    return pool.map(fetch_url, urls)
```

结果将是一个列表，其中包含每个调用返回的结果，我们可以轻松测试，这将比我们最初的示例快得多：

```
>>> timeit.timeit(fetch_all_urls_theraded, number=1)
4.660976745188236
```

# 协同程序

在大多数语言和用例中，线程是实现并发性的最常见的方式，但它们的成本很高，虽然`ThreadPool`可以很好地解决涉及数千个线程的情况，但涉及数千个线程通常是不合理的。特别是在涉及长期 I/O 的情况下，您可能很容易实现数千个并发运行的操作（想想 HTTP 服务器可能必须处理的并发 HTTP 请求的数量），而这些任务中的大多数将无所事事，大部分时间只是等待来自网络或磁盘的数据。

在这些情况下，异步 I/O 是首选方法。与同步阻塞 I/O（代码在那里等待读或写操作完成）相比，异步 I/O 允许需要数据的任务启动读操作，切换到执行其他操作，一旦数据可用，就返回到它正在执行的操作。

在某些情况下，可用数据的通知可能以信号的形式出现，这将中断并行运行的代码，但更常见的是，异步 I/O 是通过使用选择器（例如`select`、`poll`或`epoll`）来实现的以及一个事件循环，该循环将在选择器收到数据可用的通知后立即恢复等待数据的函数。

这实际上会导致交错函数能够运行一段时间，到达需要一些 I/O 的点，并将控制权传递给另一个函数，该函数将在需要执行某些 I/O 时立即返回。如果函数的执行可以通过挂起和恢复它们来交错，则称为**协程**，因为它们是协同运行的。

# 怎么做。。。

在 Python 中，协同路由通过`async def`语法实现，并通过`asyncio`事件循环执行。

例如，我们可以编写一个函数，运行两个从给定秒数开始倒计时的协同例程，打印它们的进度。这将很容易让我们看到这两个协同路由同时运行，因为我们将看到一个协同路由的输出与另一个协同路由的输出交织在一起：

```
import asyncio

async def countdown(identifier, n):
    while n > 0:
        print('left:', n, '({})'.format(identifier))
        await asyncio.sleep(1)
        n -= 1

async def main():
    await asyncio.wait([
        countdown("A", 2),
        countdown("B", 3)
    ])
```

一旦创建了一个事件循环并在其中运行`main`，我们将看到两个函数正在运行：

```
>>> loop = asyncio.get_event_loop()
>>> loop.run_until_complete(main())
left: 2 (A)
left: 3 (B)
left: 1 (A)
left: 2 (B)
left: 1 (B)
```

一旦执行完成，我们可以关闭事件循环，因为我们不再需要它：

```
>>> loop.close()
```

# 它是如何工作的。。。

我们合作项目世界的核心是**事件循环**。如果没有事件循环，就不可能运行协同路由（或者，至少它变得非常复杂），因此我们的代码要做的第一件事就是创建一个事件循环：

```
>>> loop = asyncio.get_event_loop()
```

然后，我们要求事件循环等待所提供的协同路由完成：

```
loop.run_until_complete(main())
```

`main`协同路由只启动两个`countdown`协同路由并等待它们的完成。这是通过使用`await`完成的，`asyncio.wait`函数负责等待一组协同路由：

```
await asyncio.wait([
    countdown("A", 2),
    countdown("B", 3)
])
```

`await`在这里很重要，因为我们讨论的是协同路由，所以除非显式地等待它们，否则我们的代码将立即向前移动，因此，即使我们调用了`asyncio.wait`，我们也不会等待。

在这种情况下，我们正在等待两次倒计时完成。第一次倒计时从`2`开始，由字符`A`标识，第二次倒计时从`3`开始，由`B`标识。

`countdown`函数本身非常简单。它只是一个永远循环的函数，并打印出还有多少等待。

在每个循环之间，它等待一秒钟，以便等待预期的秒数：

```
await asyncio.sleep(1)
```

您可能想知道为什么我们使用`asyncio.sleep`而不是`time.sleep`，原因是，在使用协同路由时，您必须确保将阻塞的所有其他函数也是协同路由。这样，您就知道，当您的函数被阻塞时，您会让其他协同程序继续前进。

通过使用`asyncio.sleep`，我们让事件循环在第一个`countdown`函数等待时向前移动另一个`countdown`函数，因此，我们正确地交错执行这两个函数。

这可以通过检查输出来验证。当使用`asyncio.sleep`时，输出将在两个功能之间交错：

```
left 2 (A)
left 3 (B)
left 1 (A)
left 2 (B)
left 1 (B)
```

当使用`time.sleep`时，第一个协程必须完全完成，第二个才能继续：

```
left 2 (A)
left 1 (A)
left 3 (B)
left 2 (B)
left 1 (B)
```

因此，使用协同路由的一般规则是，无论何时您要调用将阻塞的东西，请确保它也是一个协同路由，否则您将丢失协同路由的并发属性。

# 还有更多。。。

我们已经知道，协同路由最重要的好处是事件循环能够在等待 I/O 操作让其他协同路由继续进行时暂停它们的执行。虽然目前还没有支持协同路由的 HTTP 协议的内置实现，但推出一个备份版本来重现我们的示例，即同时下载一个网站，以跟踪它所花费的时间，是很容易的。

至于`ThreadPool`示例，我们需要`wait_until`函数来等待任何给定谓词为 true：

```
async def wait_until(predicate):
    """Waits until the given predicate returns True"""
    import time
    seconds = 0
    while not predicate():
        print('Waiting...')
        await asyncio.sleep(1)
        seconds += 1
    print('Done!')
    return seconds
```

我们还需要一个`fetch_url`函数来下载 URL 的内容。由于我们希望它作为一个协同程序运行，我们不能依赖于`urllib`，否则它将永远阻塞，而不是将控制权传递回事件循环。因此，我们必须使用`asyncio.open_connection`读取数据，它在纯 TCP 级别工作，因此需要我们自己实现 HTTP 支持：

```
async def fetch_url(url):
    """Fetch content of a given url from the web"""
    url = urllib.parse.urlsplit(url)
    reader, writer = await asyncio.open_connection(url.hostname, 80)
    req = ('GET {path} HTTP/1.0\r\n'
           'Host: {hostname}\r\n'
           '\r\n').format(path=url.path or '/', hostname=url.hostname)
    writer.write(req.encode('latin-1'))
    while True:
        line = await reader.readline()
        if not line.strip():
            # Read until the headers, from here on is the actualy response.
            break
    return await reader.read()
```

在这一点上，可以交错两个协同路由，并看到下载与等待同时进行，并在预期时间内完成：

```
>>> loop = asyncio.get_event_loop()
>>> t1 = asyncio.ensure_future(fetch_url('http://httpbin.org/delay/3'))
>>> t2 = asyncio.ensure_future(wait_until(t1.done))
>>> loop.run_until_complete(t2)
Waiting...
Waiting...
Waiting...
Waiting...
Done!
>>> loop.close()
>>> print('Total Time:', t2.result())
Total Time: 4
>>> print('Content:', t1.result())
Content: b'{"args":{},"data":"","files":{},"form":{},
            "headers":{"Connection":"close","Host":"httpbin.org"},
            "origin":"93.147.95.71",
            "url":"http://httpbin.org/delay/3"}\n'
```

# 过程

线程和协同路由是与 pythongil 共存的并发模型，它们利用 I/O 操作留下的执行时间来允许其他任务继续。在现代多核系统中，能够充分利用系统提供的全部功能是非常棒的，因为它涉及到真正的并行性，并将工作分配到所有可用的核上。

Python 标准库提供了非常完善的工具来处理多处理，这是在 Python 上利用并行性的一个很好的解决方案。由于多处理将导致多个单独的解释器，因此 GIL 不会成为阻碍，与线程和协程相比，将它们作为需要协作的完全隔离的进程进行推理可能更容易，而不是认为同一系统中的多个线程/协程共享底层内存状态。

管理流程的主要成本通常是生成成本，以及必须确保不会在任何异常情况下分叉子流程的复杂性，从而导致复制内存中不需要的数据或重用文件描述符。

`multiprocessing.ProcessPool`可以很好地解决所有这些问题，因为在我们的软件开始时启动一个可以确保当我们有任务提交给子流程时，我们不必支付任何特定的成本。此外，通过在开始时只创建一次流程，我们可以保证复制软件以创建子流程的状态是可预测的（并且大部分是空的）。

# 怎么做。。。

很像在*线程池*配方中，我们将需要两个函数，它们将充当在流程中并发运行的任务。

对于进程，我们实际上不需要执行 I/O 来并发运行，因此我们的任务可以做任何事情。我要使用的是在打印过程中计算斐波那契级数，这样我们就可以看到两个过程的输出是如何交错的：

```
import os

def fib(n, seen):
    if n not in seen and n % 5 == 0:
        # Print out only numbers we didn't yet compute
        print(os.getpid(), '->', n)
        seen.add(n)

    if n < 2:
        return n
    return fib(n-2, seen) + fib(n-1, seen)
```

因此，现在我们需要创建将运行`fib`函数并生成计算的多处理`Pool`：

```
>>> from multiprocessing import Pool
>>> pool = Pool()
>>> t1 = pool.apply_async(fib, args=(20, set()))
>>> t2 = pool.apply_async(fib, args=(22, set()))
>>> pool.close()
>>> pool.join()
42588 -> 20
42588 -> 10
42588 -> 0
42589 -> 20
42588 -> 5
42589 -> 10
42589 -> 0
42589 -> 5
42588 -> 15
42589 -> 15
>>> t1.get()
6765
>>> t2.get()
17711
```

您可以看到这两个进程的进程 ID 是如何交叉的，一旦作业完成，就可以得到这两个进程的结果。

# 它是如何工作的。。。

创建`multiprocessing.Pool`时，通过`os.fork`或通过生成一个新的 Python 解释器来创建相当于系统上内核数量的进程（如`os.cpu_count()`所述），具体取决于底层系统支持的内容：

```
>>> pool = Pool()
```

一旦新流程启动，它们都会做同样的事情：执行`worker`函数，从发送到`Pool`的作业队列中循环并逐个运行它们。

这意味着，如果我们创建两个流程的`Pool`，我们将有两个工人。只要我们要求`Pool`执行某项任务（通过`Pool.apply_async`、`Pool.map`或任何其他方法），作业（函数及其参数）就会被放置在`multiprocessing.SimpleQueue`中，工作人员将从中获取作业。

一旦`worker`从队列中获取任务，它将运行该任务。如果有多个`worker`实例正在运行，每个实例都将从队列中选择一个任务并运行它。

任务完成后，执行的函数的结果将被推回到结果队列中（与作业本身一起，以确定结果引用的任务），从中`Pool`将能够使用结果并将其返回到最初触发任务的代码。

所有这些通信都是跨多个进程进行的，所以不能在内存中进行。相反，作为基础的`multiprocessing.SimpleQueue`使用`pipe`，每个生产者将写入`pipe`，每个消费者将读取`pipe`。

由于`pipe`只能读写字节，我们提交给`pool`的参数和`pool`执行的函数结果通过`pickle`协议转换成字节。它能够封送/解封 Python 对象，只要双方（发送方和接收方）都有相同的模块。

因此，我们向`Pool`提交请求：

```
>>> t1 = pool.apply_async(fib, args=(20, set()))
```

`fib`函数、`20`和空集都会被酸洗并发送到队列中，供`Pool`工作人员之一使用。

同时，当工作人员拾取数据并运行斐波那契函数时，我们加入池，这样我们的主进程将阻塞，直到池中的所有进程都完成：

```
>>> pool.close()
>>> pool.join()
```

理论上，池的进程永远不会完成（它永远运行，不断地在队列中寻找要做的事情）。在呼叫`join`之前，我们先`close`游泳池。关闭池会告诉池在完成当前所做的工作后退出其所有进程。

然后，通过在`close`之后立即加入，我们等待池完成它现在正在做的事情，这就是服务我们的两个请求。

与线程一样，`multiprocessing.Pool`返回`AsyncResult`对象，这意味着我们可以通过`AsyncResult.ready()`方法检查它们的完成情况，一旦准备好，我们可以通过`AsyncResult.get()`获取返回值：

```
>>> t1.get()
6765
>>> t2.get()
17711
```

# 还有更多。。。

`multiprocessing.Pool`的工作方式与`multiprocessing.pool.ThreadPool`几乎相同。事实上，它们共享很多实现，因为其中一个是另一个的子类。

但也有一些主要差异是由所使用的底层技术造成的。一个基于线程，另一个基于子进程。

使用进程的主要好处是 Python 解释器锁不会限制它们的并行性，而且它们实际上能够彼此并行运行。

另一方面，这是有代价的。使用进程不仅在启动时更昂贵（分叉进程通常比生成线程慢），而且在使用内存方面更昂贵，因为每个进程都需要有自己的内存状态。虽然在大多数系统上，通过写时复制等技术可以大幅降低成本，但线程通常比进程便宜得多。

因此，通常最好只在应用程序开始时启动进程`pool`，这样生成进程的额外成本只需支付一次。

进程不仅启动成本更高，而且与线程相比，它们不共享程序的状态；每个进程都有自己的状态和内存。因此，不可能在`Pool`和执行任务的工人之间共享数据。所有的数据都需要通过`pickle`编码并通过`pipe`发送给另一端使用。与依赖共享队列的线程相比，这会带来巨大的成本，尤其是当必须发送的数据很大时。

因此，当参数或返回值中涉及大文件或数据时，通常最好避免涉及进程，因为这些数据必须复制多次才能到达最终目的地。在这种情况下，最好将数据保存在磁盘上，并传递文件的路径。

# 期货

生成后台任务时，它可能永远与您的主流程同时运行，并且永远不会完成自己的作业（例如，`ThreadPool`的工作线程），或者它可能是迟早会向您返回结果的内容，您可能正在等待该结果（例如，在后台下载 URL 内容的线程）。

第二类任务都有一个共同的行为：它们的结果将在`_future_`中提供。因此，一个将来可用的结果通常被称为`Future`。编程语言对未来的定义并不完全相同，Python 上的`Future`是将来将完成的任何函数，通常返回一个结果。

`Future`是可调用的本身，因此它与实际运行可调用的技术无关。您需要一种让可调用的执行继续进行的方法，在 Python 中，`Executor`提供了这种方法。

有一些执行器可以将未来运行到线程、进程或协同路由（在协同路由的情况下，循环本身就是执行器）。

# 怎么做。。。

为了经营未来，我们需要一个执行人（无论是`ThreadPoolExecutor`、`ProcessPoolExecutor`）和我们实际想要经营的期货。在我们的示例中，我们将使用一个函数返回加载网页所需的时间，以便我们可以对多个网站进行基准测试，以查看哪个网站最快：

```
import concurrent.futures
import urllib.request
import time

def benchmark_url(url):
    begin = time.time()
    with urllib.request.urlopen(url) as conn:
        conn.read()
    return (time.time() - begin, url)

class UrlsBenchmarker:
    def __init__(self, urls):
        self._urls = urls

    def run(self, executor):
        futures = self._benchmark_urls(executor)
        fastest = min([
            future.result() for future in 
                concurrent.futures.as_completed(futures)
        ])
        print('Fastest Url: {1}, in {0}'.format(*fastest))

    def _benchmark_urls(self, executor):
        futures = []
        for url in self._urls:
            future = executor.submit(benchmark_url, url)
            future.add_done_callback(self._print_timing)
            futures.append(future)
        return futures

    def _print_timing(self, future):
        print('Url {1} downloaded in {0}'.format(
            *future.result()
        ))
```

然后我们可以创建任何类型的执行人，并让我们的`UrlsBenchmarker`在其中运行其未来：

```
>>> import concurrent.futures
>>> with concurrent.futures.ThreadPoolExecutor() as executor:
...     UrlsBenchmarker([
...             'http://time.com/',
...             'http://www.cnn.com/',
...             'http://www.facebook.com/',
...             'http://www.apple.com/',
...     ]).run(executor)
...
Url http://time.com/ downloaded in 1.0580978393554688
Url http://www.apple.com/ downloaded in 1.0482590198516846
Url http://www.facebook.com/ downloaded in 1.6707532405853271
Url http://www.cnn.com/ downloaded in 7.4976489543914795
Fastest Url: http://www.apple.com/, in 1.0482590198516846
```

# 它是如何工作的。。。

`UrlsBenchmarker`将通过`UrlsBenchmarker._benchmark_urls`为每个 URL 激发未来：

```
for url in self._urls:
    future = executor.submit(benchmark_url, url)
```

每个 future 将执行`benchmark_url`，下载给定 URL 的内容并返回下载所用的时间以及 URL 本身：

```
def benchmark_url(url):
    begin = time.time()
    # download url here...
    return (time.time() - begin, url)
```

返回 URL 本身是必要的，因为`future`可以知道其返回值，但不知道其参数。因此，一旦我们`submit`函数，我们就失去了它所关联的 URL，并且通过将其与时间一起返回，我们将始终可以在时间出现时使用 URL。

然后通过`future.add_done_callback`为每个`future`添加回调：

```
future.add_done_callback(self._print_timing)
```

一旦 future 完成，它将调用`UrlsBenchmarker._print_timing`，打印运行 URL 所需的时间。这将通知用户基准测试正在进行，并已完成其中一个 URL。

然后，`UrlsBenchmarker._benchmark_urls`将返回`futures`用于我们必须在列表中进行基准测试的所有 URL。

然后将该列表传递给`concurrent.futures.as_completed`。这将创建一个迭代器，该迭代器将按照完成的顺序返回所有`futures`，并且仅在完成时返回。因此，我们知道，通过对其进行迭代，我们将只获取已经完成的`futures`，一旦消耗了所有已经完成的`futures`，我们将阻止等待新未来的完成：

```
[
    future.result() for future in 
        concurrent.futures.as_completed(futures)
]
```

因此，只有当所有`futures`都完成时，循环才会结束。

完成的`futures`列表由`list`理解使用，该理解将创建一个包含`futures`结果的列表。

由于结果都是（`time`、`url`形式，我们可以使用`min`以最短的时间抓取结果，即下载时间较短的 URL。

这是因为比较两个元组会按顺序比较元素：

```
>>> (1, 5) < (2, 0)
True
>>> (2, 1) < (0, 5)
False
```

因此，在元组列表上调用`min`将获取元组第一个元素中具有最小值的条目：

```
>>> min([(1, 2), (2, 0), (0, 7)])
(0, 7)
```

只有当有两个具有相同值的第一个元素时，才会查看第二个元素：

```
>>> min([(0, 7), (1, 2), (0, 3)])
(0, 3)
```

因此，我们以最短的时间获取 URL（因为时间是未来返回的元组中的第一个条目），并以最快的速度打印它：

```
fastest = min([
    future.result() for future in 
        concurrent.futures.as_completed(futures)
])
print('Fastest Url: {1}, in {0}'.format(*fastest))
```

# 还有更多。。。

期货执行人与`multiprocessing.pool`提供的员工池非常相似，但他们有一些差异，可能会将您推向一个或另一个方向。

主要的区别可能是工人的开始方式。池启动固定数量的工作线程，这些工作线程在创建池时同时创建和启动。因此，尽早创建池会降低在应用程序开始时生成工作线程的成本。这意味着应用程序的启动速度可能非常慢，因为它可能需要根据您请求的工作人员数量或系统的内核数量来分叉许多进程。相反，执行者只在需要工人的时候才创造工人，这意味着在未来发展，以避免在有可用工人时创造新工人。

因此，执行者通常会更快地启动，但在首次向其发送期货时会有更多的延迟，而池的大部分成本都集中在启动时间上。因此，如果您经常需要创建和销毁工作进程池，那么`futures`执行器可以更高效地使用。

# 计划任务

一种常见的后台任务是在任何给定时间都应该在后台自行运行的操作。通常，通过 cron 守护程序或类似的系统工具，通过配置守护程序在提供的时间运行给定的 Python 脚本来管理这些脚本。

当您的主应用程序需要周期性地执行任务（例如，使缓存过期、重置密码链接、刷新要发送的电子邮件队列或类似任务）时，通过 cron 作业这样做实际上是不可行的，因为您需要将数据转储到另一个进程可以访问的地方：磁盘上、数据库上，或任何类似的共享存储。

幸运的是，Python 标准库提供了一种简单的方法来调度在任何给定时间执行并与线程连接的任务。对于计划的后台任务，它是一个非常简单有效的解决方案。

# 怎么做。。。

`sched`模块提供了一个功能齐全的调度任务执行器，我们可以将其与线程混合，创建一个后台调度程序：

```
import threading
import sched
import functools

class BackgroundScheduler(threading.Thread):
    def __init__(self, start=True):
        self._scheduler = sched.scheduler()
        self._running = True
        super().__init__(daemon=True)
        if start:
            self.start()

    def run_at(self, time, action, args=None, kwargs=None):
        self._scheduler.enterabs(time, 0, action, 
                                argument=args or tuple(), 
                                kwargs=kwargs or {})

    def run_after(self, delay, action, args=None, kwargs=None):
        self._scheduler.enter(delay, 0, action, 
                            argument=args or tuple(), 
                            kwargs=kwargs or {})

    def run_every(self, seconds, action, args=None, kwargs=None):
        @functools.wraps(action)
        def _f(*args, **kwargs):
            try:
                action(*args, **kwargs)
            finally:
                self.run_after(seconds, _f, args=args, kwargs=kwargs)
        self.run_after(seconds, _f, args=args, kwargs=kwargs)

    def run(self):
        while self._running:
            delta = self._scheduler.run(blocking=False)
            if delta is None:
                delta = 0.5
            self._scheduler.delayfunc(min(delta, 0.5))

    def stop(self):
        self._running = False
```

可以启动`BackgroundScheduler`并向其添加作业，以便在固定时间开始执行作业：

```
>>> import time
>>> s = BackgroundScheduler()
>>> s.run_every(2, lambda: print('Hello World'))
>>> time.sleep(5)
Hello World
Hello World
>>> s.stop()
>>> s.join()
```

# 它是如何工作的。。。

`BackgroundScheduler`子类`threading.Thread`以便在应用程序执行其他操作时在后台运行。注册的任务将在次线程中启动并执行，而不会妨碍主代码：

```
class BackgroundScheduler(threading.Thread):
        def __init__(self):
            self._scheduler = sched.scheduler()
            self._running = True
            super().__init__(daemon=True)
            self.start()
```

每当`BackgroundScheduler`被创建时，它的线程也会启动，因此它会立即可用。线程将在`daemon`模式下运行，这意味着如果程序结束时仍在运行，它不会阻止程序退出。

通常，Python 在退出应用程序时会等待所有线程，因此将线程设置为`daemon`线程可以在不等待线程的情况下退出。

`threading.Thread`执行`run`方法作为线程代码。在我们的例子中，它是一种反复运行调度器中注册的任务的方法：

```
def run(self):
    while self._running:
        delta = self._scheduler.run(blocking=False)
        if delta is None:
            delta = 0.5
        self._scheduler.delayfunc(min(delta, 0.5))
```

`_scheduler.run(blocking=False)`表示从计划任务中选择一个要运行的任务并运行。然后，它返回在运行下一个任务之前仍需等待的时间。如果没有返回时间，则表示没有要运行的任务。

通过`_scheduler.delayfunc(min(delta, 0.5))`等待下一个任务需要运行的时间，最多半秒。

我们最多等待半秒钟，因为在等待时，计划的任务可能会更改。一个新任务可能会被注册，我们希望确保它不必等待超过半秒，调度器就可以捕获它。

如果我们在下一个任务之前等待的时间正好是等待的时间，我们可能会运行一次，返回到下一个任务在 60 秒后，然后开始等待 60 秒。但是，如果在我们等待时，用户注册了一个必须在 5 秒内运行的新任务，该怎么办？无论如何，我们会在 60 秒内运行它，因为我们已经在等待了。通过最多等待 0.5 秒，我们知道将需要半秒来完成下一个任务，并且它将在 5 秒钟内正常运行。

等待时间少于下一个任务之前等待的时间不会使任务运行得更快，因为计划程序不会运行任何尚未超过其计划时间的任务。因此，如果没有任务要运行，调度程序会不断地告诉我们，*您必须等待*，我们将等待半秒，等待的次数与到达下一个计划任务的计划时间所需的次数相同。

`run_at`、`run_after`和`run_every`方法实际上是注册函数以在特定时间执行的方法。

`run_at`和`run_after`只是简单地包装调度器的`enterabs`和`enter`方法，它允许我们注册一个在特定时间或*n*秒之后运行的任务。

最有趣的功能可能是`run_every`，它每隔*n*秒反复运行一个任务：

```
def run_every(self, seconds, action, args=None, kwargs=None):
    @functools.wraps(action)
    def _f(*args, **kwargs):
        try:
            action(*args, **kwargs)
        finally:
            self.run_after(seconds, _f, args=args, kwargs=kwargs)
    self.run_after(seconds, _f, args=args, kwargs=kwargs)
```

该方法获取必须运行的可调用函数，并将其包装到实际运行该函数的装饰器中，但一旦完成，它会将该函数安排回重新执行。这样，它将一次又一次地运行，直到调度程序停止，并且每当它完成时，都会再次调度它。

# 在进程之间共享数据

当使用线程或协程时，由于它们共享相同的内存空间，所以数据在它们之间共享。因此，您可以从任何线程访问任何对象，只要注意避免争用条件并提供适当的锁定。

相反，在流程中，事情变得更加复杂，没有数据在它们之间共享。因此，当使用`ProcessPool`或`ProcessPoolExecutor`时，我们需要找到一种方法来跨进程传递数据，并使它们能够共享一个公共状态。

Python 标准库提供了许多工具来创建进程之间的通信通道：`multiprocessing.Queues`、`multiprocessing.Pipe`、`multiprocessing.Value`和`multiprocessing.Array`可用于创建一个进程可以提供而另一个进程可以使用的队列，或者只是在共享内存中的多个进程之间共享值。

虽然所有这些都是可行的解决方案，但它们有一些限制：在创建任何流程之前必须创建所有共享值，因此，如果共享值的数量是可变的，并且它们在可存储的类型方面是有限的，则它们就不可行。

相反，`multiprocessing.Manager`允许我们通过共享`Namespace`存储任意数量的共享值。

# 怎么做。。。

以下是此配方的步骤：

1.  `manager`应该在应用程序开始时创建，然后所有进程都可以从中设置和读取值：

```
import multiprocessing

manager = multiprocessing.Manager()
namespace = manager.Namespace()
```

2.  一旦我们有了`namespace`，任何流程都可以为其设置值：

```
def set_first_variable():
    namespace.first = 42
p = multiprocessing.Process(target=set_first_variable)
p.start()
p.join()

def set_second_variable():
    namespace.second = dict(value=42)
p = multiprocessing.Process(target=set_second_variable)
p.start()
p.join()

import datetime
def set_custom_variable():
    namespace.last = datetime.datetime.utcnow()
p = multiprocessing.Process(target=set_custom_variable)
p.start()
p.join()
```

3.  任何进程都可以访问它们：

```
>>> def print_variables():
...    print(namespace.first, namespace.second, namespace.last)
...
>>> p = multiprocessing.Process(target=print_variables)
>>> p.start()
>>> p.join()
42 {'value': 42} 2018-05-26 21:39:17.433112
```

无需在主流程早期或从主流程创建变量，所有流程将能够读取或设置任何变量，只要它们有权访问`Namespace`。

# 它是如何工作的。。。

`multiprocessing.Manager`类充当服务器，能够存储任何进程可以访问的值，该进程引用了`Manager`及其想要访问的值。

`Manager`本身可以通过知道它正在侦听的套接字或管道的地址来访问，每个引用`Manager`实例的进程都知道：

```
>>> manager = multiprocessing.Manager()
>>> print(manager.address)
/tmp/pymp-4l33rgjq/listener-34vkfba3
```

然后，一旦您知道如何联系管理者本身，您就需要能够告诉管理者您要从管理者正在管理的所有对象中访问哪个对象。

这可以通过`Token`表示并精确定位该对象来实现：

```
>>> namespace = manager.Namespace()
>>> print(namespace._token)
Token(typeid='Namespace', 
      address='/tmp/pymp-092482xr/listener-yreenkqo', 
      id='7f78c7fd9630')
```

特别是，`Namespace`是一种允许我们在其中存储任何变量的对象。因此，它仅使用`namespace`标记就可以访问`Namespace`中存储的任何内容。

所有进程，因为它们是从同一个原始进程复制的，具有名称空间标记和管理器地址的，都能够访问`namespace`，从而从中设置或读取值。

# 还有更多。。。

`multiprocessing.Manager`不限于使用源自同一流程的流程。

可以创建一个在网络上侦听的`Manager`，这样任何能够连接到它的进程都可以访问它的内容：

```
>>> import multiprocessing.managers
>>> manager = multiprocessing.managers.SyncManager(
...     address=('localhost', 50000), 
...     authkey=b'secret'
... )
>>> print(manager.address)
('localhost', 50000)
```

然后，服务器启动后：

```
>>> manager.get_server().serve_forever()
```

其他进程将能够通过创建一个`manager2`实例来连接到它，该实例与它们想要连接的管理器的参数完全相同，然后显式连接：

```
>>> manager2 = multiprocessing.managers.SyncManager(
...     address=('localhost', 50000), 
...     authkey=b'secret'
... )
>>> manager2.connect()
```

让我们在 manager 中创建一个`namespace`并在其中设置一个值：

```
>>> namespace = manager.Namespace()
>>> namespace.value = 5
```

知道`namespace`的令牌值，可以创建一个代理对象，通过网络从`manager2`访问`namespace`：

```
>>> from multiprocessing.managers import NamespaceProxy
>>> ns2 = NamespaceProxy(token, 'pickle', 
...                      manager=manager2, 
...                      authkey=b'secret')
>>> print(ns2.value)
5
```