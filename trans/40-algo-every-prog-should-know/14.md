# 实际考虑

本书中介绍了一系列算法，可用于解决实际问题。本章是关于本书中介绍的算法的一些实际注意事项。

本章组织如下。我们将从介绍开始。然后，我们将介绍算法的可解释性这一重要主题，即算法的内部机制可以用可理解的术语解释的程度。然后，我们将介绍使用算法的道德规范以及在实现算法时产生偏见的可能性。接下来，讨论了处理 NP 难问题的技术。最后，我们将研究在选择算法之前应该考虑的因素

在本章结束时，您将了解使用算法时必须牢记的实际注意事项

在本章中，我们将介绍以下主题：

*   介绍实际考虑
*   算法的可解释性
*   理解伦理和算法
*   减少模型中的偏差
*   解决 NP 难问题
*   何时使用算法

让我们从介绍开始，

# 介绍实际考虑

除了设计、开发和测试算法，在许多情况下，重要的是考虑某些实际的方面，开始依赖于机器来解决真实世界的问题，因为这使得解决方案更有用。对于某些算法，我们可能需要考虑的方法，以可靠地纳入新的重要信息，预计将继续变化，甚至在我们已经部署了我们的算法。合并这些新信息会以任何方式改变我们经过良好测试的算法的质量吗？如果是的话，我们的设计是如何处理的？然后，对于一些使用全球模式的算法，我们可能需要关注捕捉全球地缘政治局势变化的实时参数。此外，在一些用例中，我们可能需要考虑在使用时强制执行的解决方案是有用的。

When we are using algorithms to solve a real-world problem, we are, in a way, relying on machines for problem solving. Even the most sophisticated algorithms are based on simplification and assumptions and cannot handle surprises. We are still not even close to fully handing over critical decision making to our own designed algorithms.

例如，由于隐私问题，谷歌设计的推荐引擎算法最近面临欧盟的监管限制。这些算法可能是其领域中最先进的一些算法。但如果被禁止，这些算法实际上可能会变得毫无用处，因为它们无法用来解决它们应该解决的问题。

问题的真相是，不幸的是，算法的实际考虑仍然是在最初设计阶段通常不考虑的事后考虑。对于许多用例，一旦部署了算法并且提供解决方案的短期兴奋感结束，使用算法的实际方面和含义将随着时间的推移而被发现，并将确定项目的成功或失败

让我们来看看一个实际的例子，在这个例子中，世界上最好的 IT 公司之一设计的一个引人注目的项目由于没有注意到实际考虑而失败了。

# AI 推特机器人的悲惨故事

让我们介绍一下 Tay 的经典示例，它是微软在 2016 年创建的第一个人工智能推特机器人。Tay 由人工智能算法操作，应该从环境中学习并不断改进自己。不幸的是，在网络空间生活了几天后，泰伊开始从持续推文的种族主义和粗鲁中学习。它很快就开始撰写自己的攻击性推文。尽管它展示了智慧，并很快学会了如何根据设计的实时事件创建定制推文，但同时它严重冒犯了人们。微软将其离线，并试图对其进行重新加工，但没有成功。微软最终不得不终止这个项目。这是一个雄心勃勃的项目的悲惨结局。

请注意，尽管微软内置的智能令人印象深刻，但该公司忽略了部署自学习 Twitter 机器人的实际意义。NLP 和机器学习算法可能是同类中最好的，但由于明显的缺点，它实际上是一个无用的项目。如今，由于忽视了允许算法动态学习的实际意义，Tay 已经成为一个失败的教科书例子。从 Tay 的失败中吸取的教训无疑影响了后来的人工智能项目。数据科学家也开始更加关注算法的透明度。这就引出了下一个主题，它探讨了使算法透明的必要性和方法。

# 算法的可解释性

黑盒算法是一种其逻辑无法被人类解释的算法，无论是由于其复杂性还是由于其逻辑以复杂的方式表示。另一方面，白盒算法的逻辑对人类来说是可见和可理解的。换句话说，可解释性有助于人脑理解一个算法为什么会给出特定的结果。可解释性的程度是衡量一个特定算法对人脑来说是可理解的程度。许多类别的算法，特别是与机器学习相关的算法，被归类为黑盒。如果算法用于关键决策，了解算法生成结果背后的原因可能很重要。将黑盒算法转换为白盒算法也能更好地洞察模型的内部工作原理。一个可解释的算法将指导医生实际使用哪些特征来分类患者是否生病。如果医生对结果有任何疑问，他们可以回去重新检查这些特征的准确性

# 机器学习算法与可解释性

算法的可解释性对于机器学习算法具有特殊的重要性。在机器学习的许多应用中，要求用户信任模型以帮助他们做出决策。可解释性在此类用例中需要时提供透明度

让我们更深入地看一个具体的例子。让我们假设我们想要使用机器学习来根据波士顿地区的房价特征预测房价。我们还假设，只有当我们能够在需要时提供详细信息来证明任何预测的合理性时，当地的城市法规才允许我们使用机器学习算法。这些信息用于审计目的，以确保房地产市场的某些部分不被人为操纵。使我们经过培训的模型可解释将提供这些额外信息。

让我们研究一下可用于实现经过培训的模型的可解释性的不同选项。

# 提出解释性策略

对于机器学习，基本上有两种策略可以为算法提供解释性：

*   **一个全局可解释性策略：**这是提供一个整体模型的制定细节。
*   **局部可解释性策略：**这是为我们训练的模型所做的一个或多个单独预测提供基本原理。

对于全局可解释性，我们采用了概念激活向量（**TCAV**）的**测试等技术，用于为图像分类模型提供可解释性。TCAV 依靠计算方向导数来量化用户定义概念与图片分类之间的关系程度。例如，它将量化将一个人分类为男性的预测对图片中是否存在面部毛发的敏感程度。还有其他全局可解释性策略，如**部分依赖图**和计算**排列重要性**，这有助于解释我们训练模型中的公式。全局和局部可解释性策略既可以是模型特定的，也可以是模型不可知的。特定于模型的策略适用于某些类型的模型，而模型不可知策略可以应用于各种各样的模型。**

下图总结了机器学习可解释性的不同策略：

![](assets/508ea77c-e398-4c0a-a06d-d872d52423d3.png)

现在，让我们看看如何使用这些策略之一实现可解释性。

# 实现可解释性

**局部可解释模型不可知解释**（**LIME**是一种模型不可知方法，可以解释训练模型做出的单个预测。由于模型不可知，它可以解释大多数类型的训练机器学习模型的预测。

莱姆通过对每个实例的输入进行小的更改来解释决策。它可以收集对该实例的局部决策边界的影响。它迭代循环以提供每个变量的详细信息。查看输出，我们可以看到哪个变量对该实例的影响最大

让我们看看如何使用石灰来解释我们的房价模型的个别预测：

1.  如果您以前从未使用过 LIME，则需要使用`pip`安装软件包：

```py
!pip install lime
```

2.  然后，让我们导入我们需要的 Python 包：

```py
import sklearn as sk
import numpy as np
from lime.lime_tabular import LimeTabularExplainer as ex
```

3.  我们将训练一个能够预测特定城市房价的模型。为此，我们将首先导入存储在`housing.pkl`文件中的数据集。然后，我们将探讨它的功能：

![](assets/df39d111-43cb-4129-90b9-61e991744bd4.png)

根据这些特征，我们需要预测房屋的价格。

4.  现在，让我们来训练模型。我们将使用随机森林回归器来训练模型。首先，我们将数据划分为测试和训练分区，然后使用它来训练模型：

```py
from sklearn.ensemble import RandomForestRegressor
X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    housing.data, housing.target)

regressor = RandomForestRegressor()
regressor.fit(X_train, y_train)
```

5.  接下来，让我们确定类别列：

```py
cat_col = [i for i, col in enumerate(housing.data.T)
                        if np.unique(col).size < 10]
```

6.  现在，让我们用所需的配置参数实例化 LIME 解释程序。请注意，我们指定我们的标签为`'price'`，代表波士顿的房价：

```py
myexplainer = ex(X_train,
    feature_names=housing.feature_names,
    class_names=['price'],
    categorical_features=cat_col,
    mode='regression')
```

7.  让我们试着研究一下预测的细节。首先，让我们从 matplotlib 导入 pyplot 作为绘图仪

```py
exp = myexplainer.explain_instance(X_test[25], regressor.predict,
        num_features=10)

exp.as_pyplot_figure()
from matplotlib import pyplot as plt
plt.tight_layout()
```

8.  当石灰解释者处理单个预测时，我们需要选择要分析的预测。我们已询问解释人其对索引为`1`和`35`的预测的合理性：

![](assets/68e5f21f-d5d7-4bd4-bd7a-4e78b13b8c87.png)

让我们试着用石灰来分析前面的解释，它告诉我们以下几点：

*   **单个预测**中使用的特征列表：它们在前面的屏幕截图中的*y*轴上显示。
*   **特征在决定**时的相对重要性：条线越大，重要性越大。数字的值位于*x*轴上。
*   **标签**上每个输入特征的正面或负面影响：红色条表示负面影响，绿色条表示特定特征的正面影响。

# 理解伦理和算法

通过算法形成模式可能直接或间接导致不道德的决策。在设计算法时，很难预见潜在道德影响的全部范围，特别是对于大型算法，其中可能有多个用户参与设计。这使得分析人类主体性的影响变得更加困难。

More and more companies are making the ethical analysis of an algorithm part of its design. But the truth is that the problems may not become apparent until we find a problematic use case. 

# 学习算法的问题

能够根据不断变化的数据模式进行自我微调的算法称为**学习****算法**。他们处于实时学习模式，但这种实时学习能力可能具有道德含义。这就产生了这样一种可能性，即他们的学习可能会导致从伦理角度来看可能存在问题的决策。由于他们被创造为处于持续进化阶段，因此几乎不可能持续对他们进行伦理分析。

As the complexity of algorithms grows, it is becoming more and more difficult to fully understand their long-term implications for individuals and groups within society.

# 理解伦理考虑

算法解决方案是没有心脏的数学公式。负责开发算法的人员有责任确保他们符合我们试图解决的问题的道德敏感性。算法的这些伦理考虑取决于算法的类型。

例如，让我们研究一下以下算法及其伦理考虑。需要仔细考虑道德因素的强大算法示例如下：

*   分类算法在社会上使用时，决定了个人和群体是如何形成和管理的
*   在推荐引擎中使用的算法可以将简历与求职者（包括个人和群体）进行匹配。
*   数据挖掘算法用于从用户处挖掘信息，并提供给决策者和政府。
*   政府开始使用机器学习算法来授予或拒绝申请人签证。

因此，算法的伦理考虑将取决于用例及其直接或间接影响的实体。在开始使用关键决策算法之前，需要从伦理角度进行仔细分析。在接下来的部分中，我们将看到在仔细分析算法时应牢记的因素。

# 非决定性证据

用于训练机器学习算法的数据可能没有确凿的证据。例如，在临床试验中，由于可用证据有限，可能无法证明药物的有效性。同样，可能有有限的非决定性证据表明，某个城市的某个邮政编码更有可能参与欺诈。当我们根据使用有限数据的算法发现的数学模式来判断决策时，我们应该小心

Decisions that are based on inconclusive evidence are prone to lead to unjustified actions.

# 可追溯性

在机器学习算法中，训练阶段和测试阶段之间的脱节意味着，如果某个算法造成了一些危害，则很难进行跟踪和调试。此外，当在算法中发现问题时，很难实际确定受其影响的人

# 误导性证据

算法是数据驱动的公式。**垃圾输入、垃圾输出**（**GIGO**原则意味着算法的结果只能与它们所基于的数据一样可靠。如果数据中存在偏差，它们也将反映在算法中。

# 不公平结果

使用算法可能会损害已经处于不利地位的脆弱社区和群体

此外，使用算法分配研究资金已不止一次地被证明偏向于男性群体。用于批准移民的算法有时无意中偏向于弱势群体

尽管使用了高质量的数据和复杂的数学公式，但如果结果不公平，整个努力可能弊大于利。

# 减少模型中的偏差

在当今世界，存在着基于性别、种族和性取向的众所周知、有据可查的普遍偏见。这意味着我们收集的数据预计会表现出这些偏见，除非我们正在处理一个在收集数据之前已经努力消除这些偏见的环境。

算法中的所有偏差都直接或间接地归因于人类偏差。人类偏见既可以反映在算法使用的数据中，也可以反映在算法本身的公式中。对于遵循**CRISP-DM**（简称**跨行业标准流程**）生命周期的典型机器学习项目，如[第 5 章](05.html)、*图形算法*所述，偏差如下：

![](assets/2ae7ab7a-9d40-4e63-97a0-13ce65e941e8.png)

减少偏见最棘手的部分是首先识别和定位无意识偏见

# 解决 NP 难问题

NP 难问题在[第 4 章](04.html)、*设计算法*中进行了广泛讨论。一些 NP 难问题很重要，我们需要设计算法来解决它们。

如果由于 NP 难问题的复杂性或可用资源的限制而无法找到解决方案，我们可以采用以下方法之一：

*   简化问题
*   为类似问题定制众所周知的解决方案
*   使用概率方法

让我们逐一调查一下。

# 简化问题

我们可以根据某些假设来简化这个问题。解决的问题仍然给出了一个不完美的解决方案，但仍然是有洞察力和有用的。为了使这一点起作用，所选择的假设应该尽可能不受限制。

# 实例

回归问题中特征和标签之间的关系很少是完全线性的。但在我们通常的操作范围内，它可能是线性的。将关系近似为线性大大简化了算法，并被广泛使用。但这也引入了一些影响算法精度的近似值。应仔细研究近似值和准确性之间的权衡，并选择适合利益相关者的适当平衡。

# 为类似问题定制众所周知的解决方案

如果已知类似问题的解决方案，则可以将该解决方案用作起点。机器学习中的**迁移学习**（**TL**）概念就是基于这一原理。其思想是使用已经预先训练好的模型的推理作为训练算法的起点。

# 实例

让我们假设我们想要训练一个二进制分类器，在公司培训期间使用计算机视觉，基于实时视频馈送区分苹果和 Windows 笔记本电脑。从视频源来看，模型开发的第一阶段是检测不同的对象，并确定哪些对象是笔记本电脑。一旦完成，我们可以进入制定规则的第二阶段，以区分苹果和 Windows 笔记本电脑

现在，已经有了经过良好训练、经过良好测试的开源模型，可以处理模型训练的第一阶段。为什么不把它们作为一个起点，并将推论用于第二阶段，即区分 Windows 和 Apple 笔记本电脑？这将给我们一个跳跃式的开始，解决方案将不那么容易出错，因为第 1 阶段已经过很好的测试。

# 使用概率方法

我们使用概率方法来得到一个合理的好的解决方案，这是可行的，但不是最优的。当我们使用[第 7 章](07.html)*传统监督学习算法*中的决策树算法来解决给定问题时，解决方案基于概率方法。我们没有证明这是一个最佳解决方案，但这是一个相当好的解决方案，为我们试图在需求中定义的约束范围内解决的问题提供了一个有用的答案。

# 实例

许多机器学习算法从随机解开始，然后迭代改进该解。最终的解决方案可能是有效的，但我们无法证明它是最好的。此方法用于复杂问题，以便在合理的时间范围内解决这些问题。这就是为什么对于许多机器学习算法来说，获得可重复结果的唯一方法是使用相同的种子使用相同的随机数序列。

# 何时使用算法

算法就像从业者工具箱中的工具。首先，我们需要了解在特定情况下使用哪种工具是最好的。有时，我们需要问问自己，对于我们试图解决的问题，我们是否有一个解决方案，以及何时是部署解决方案的正确时机。我们需要确定使用一种算法是否能够为实际问题提供一个实际有用的解决方案，而不是替代方案。我们需要从三个方面分析使用该算法的效果：

*   **成本**：是否可以通过使用来证明与实现算法的努力相关的成本？
*   **时间**：我们的解决方案是否使整个流程比简单的备选方案更高效？
*   **准确度**：我们的解决方案是否比简单的方案产生更准确的结果？

要选择正确的算法，我们需要找到以下问题的答案：

*   我们可以通过假设来简化问题吗？
*   我们将如何评估我们的算法？关键指标是什么？
*   它将如何部署和使用？
*   这需要解释吗？
*   我们是否了解安全性、性能和可用性这三个重要的非功能性需求？
*   有没有预期的最后期限？

# 一个实际的例子——黑天鹅事件

算法输入数据，处理和表达数据，并解决问题。如果收集到的数据是关于一个非常有影响和非常罕见的事件呢？我们如何利用该事件和可能导致大爆炸的事件产生的数据的算法？让我们在本节中研究这方面。

2001 年，纳西姆·塔勒布（Nassim Taleb）在其著作《被随机性愚弄》中以*黑天鹅事件*为代表，这类事件极为罕见

Before black swans were first discovered in the wild, for centuries, they were used to represent something that cannot happen. After their discovery, the term remained popular but there was a change in what it represents. It now represents something so rare that it cannot be predicted.

Taleb 提供了这四个标准来将事件归类为黑天鹅事件。

# 将事件归类为黑天鹅事件的四个标准

决定一个罕见的事件是否应该被归类为黑天鹅事件有点棘手。一般来说，为了被归类为黑天鹅，它应该满足以下四个标准。

1.  首先，一旦事件发生，对观察家来说一定是一个令人震惊的惊喜，例如，在广岛投下原子弹。
2.  这一事件应该是一场轰动一时的事件——一场破坏性事件，一场重大事件，比如西班牙流感的爆发。
3.  一旦事件发生，尘埃落定，作为观察小组一部分的数据科学家应该意识到，事实上这并没有那么令人惊讶。观察者从未注意到一些重要线索。如果他们有能力和主动性，黑天鹅事件本来是可以预测的。例如，西班牙流感爆发的一些线索在成为全球爆发之前就被忽视了。此外，在原子弹实际落在广岛之前，曼哈顿项目已经运行了多年。观察组的人就是不能把这些点连起来。

4.  当它发生时，虽然黑天鹅事件的观察家们得到了他们一生中的惊喜，但对某些人来说，这可能根本不是一个惊喜。例如，对于多年来致力于研制原子弹的科学家来说，使用原子能从来都不是一个意外事件，而是一个意料之中的事件。

# 算法在黑天鹅事件中的应用

黑天鹅事件的主要方面与算法有关：

*   有许多复杂的预测算法可用。但是，如果我们希望使用标准的预测技术来预测黑天鹅事件作为预防措施，这是行不通的。使用这种预测算法只会提供虚假的安全性。
*   一旦黑天鹅事件发生，预测其对更广泛的社会领域（包括经济、公众和政府问题）的确切影响通常是不可能的。首先，作为一个罕见的事件，我们没有正确的数据提供给算法，我们也没有掌握我们可能从未探索和理解的更广泛的社会领域之间的关联和互动
*   需要注意的一点是，黑天鹅事件不是随机事件。我们只是没有能力关注最终导致这些事件的复杂事件。这是一个算法可以发挥重要作用的领域。我们应该确保，在未来，我们有一个预测和检测这些小事件的策略，随着时间的推移，这些小事件结合在一起，形成了黑天鹅事件。

The COVID-19 outbreak of early 2020 is the best example of a black swan event of our times. 

前面的例子表明，首先考虑和理解我们正在试图解决的问题的细节是多么重要，然后提出通过实现基于算法的解决方案我们可以为解决方案做出贡献的领域。如前所述，如果不进行全面分析，使用算法可能只能解决复杂问题的一部分，达不到预期效果。

# 总结

在本章中，我们了解了设计算法时应考虑的实际方面。我们研究了算法可解释性的概念，以及我们在不同层次上提供算法可解释性的各种方式。我们还研究了算法中潜在的伦理问题。最后，我们描述了在选择算法时要考虑哪些因素。

算法是我们今天所见证的自动化新世界的引擎。了解、试验和理解使用算法的含义非常重要。了解他们的优势和局限性，以及使用算法的道德含义，将大大有助于使这个世界成为一个更美好的生活场所。这本书是为了在这个不断变化和演变的世界中实现这一重要目标而做出的努力