# 使用编译的 GPU 代码

在本书的整个过程中，我们通常依赖 PyCUDA 库为我们自动接口我们的内联 CUDA-C 代码，使用即时编译并与 Python 代码链接。然而，我们可能还记得，有时编译过程可能需要一段时间。在[第 3 章](03.html)*PyCUDA*入门中，我们甚至详细了解了编译过程是如何导致减速的，甚至可以任意决定何时编译和保留内联代码。在某些情况下，考虑到应用程序，这可能会带来不便和麻烦，甚至在实时系统的情况下，这是不可接受的

为此，我们将最终了解如何使用来自 Python 的预编译 GPU 代码。特别是，我们将研究三种不同的方法来实现这一点。首先，我们将看看如何通过编写一个主机端 CUDA-C 函数来实现这一点，该函数可以间接启动 CUDA 内核。此方法将涉及使用标准 Python Ctypes 库调用主机端函数。第二，我们将把内核编译成一个 PTX 模块，它实际上是一个包含已编译二进制 GPU 的 DLL 文件。然后我们可以用 PyCUDA 加载这个文件，并直接启动内核。最后，我们将在本章结束时介绍如何将我们自己的完整 Ctypes 接口编写到 CUDA 驱动程序 API。然后，我们可以使用驱动程序 API 中的适当函数来加载我们的 PTX 文件并启动内核。

本章的学习成果如下：

*   使用 Ctypes 模块启动编译（主机端）代码
*   使用主机端 CUDAC 包装器和 Ctypes 从 Python 启动内核
*   如何将 CUDA C 模块编译成 PTX 文件
*   如何将 PTX 模块加载到 PyCUDA 中以启动预编译内核
*   如何将自己的自定义 Python 接口写入 CUDA 驱动程序 API

# 使用 Ctypes 启动编译代码

现在，我们将简要概述 Python 标准库中的 Ctypes 模块。Ctypes 用于从 Linux`.so`（共享对象）或 Windows 调用函数。DLL（动态链接库）预编译的二进制文件。这将允许我们突破纯 Python 的世界，并与已编译的语言和代码（特别是 C 和 C++）进行接口。Nvidia 只提供与我们的 CUDA 设备接口的这种预编译二进制文件，因此如果我们想回避 PycUDA，我们将不得不使用 CyType。

让我们从一个非常基本的例子开始：我们将向您展示如何直接从 Ctypes 调用`printf`。打开 IPython 的一个实例并键入`import ctypes`。我们现在要看看如何从 Ctypes 调用标准的`printf`函数。首先，我们必须导入适当的库：在 Linux 中，通过键入`libc = ctypes.CDLL('libc.so.6')`加载 LibC 库（在 Windows 中，将`'libc.so.6'`替换为`'msvcrt.dll'`）。我们现在可以通过输入`libc.printf("Hello from ctypes!\n")`从 IPython 提示符直接调用`printf`。你自己试试吧！

现在让我们尝试其他方法：输入来自 IPython 的`libc.printf("Pi is approximately %f.\n", 3.14)`；你应该得到一个错误。这是因为`3.14`不是从 Python 浮点变量到 C 双变量的适当类型转换，我们可以使用如下的 CType：

```py
libc.printf("Pi is approximately %f.\n", ctypes.c_double(3.14)) 
```

输出应如预期的那样。就像从 PyCUDA 启动 CUDA 内核一样，我们必须同样小心地将输入类型转换为具有 Ctypes 的函数。

Always be sure to appropriately typecast inputs into any function that you call with Ctypes from Python to the appropriate C datatypes (in Ctypes, these are preceded by c_: `c_float`, `c_double`, `c_char`, `c_int`, and so on).

# 曼德尔布罗特集（再次）重访

让我们回顾一下我们在[第 1 章](01.html)中看到的 Mandelbrot 集合，*为什么要进行 GPU 编程？*、[第三章](03.html)、*PyCUDA*入门。首先，我们将编写一个完整的 CUDA 内核，该内核将在给定一组特定参数的情况下计算 Mandelbrot 集，并提供一个适当的主机端包装函数，稍后我们可以从 Ctypes 进行接口。我们将首先将这些函数写入单个 CUDA-C`.cu`源文件，然后使用 NVCC 编译器将其编译成 DLL 或`.so`二进制文件。最后，我们将编写一些 Python 代码，以便运行二进制代码并显示 Mandelbrot 集。

现在，我们将应用我们对 Ctypes 的了解，从 Python 启动一个预编译的 CUDA 内核，而不需要 PyCUDA 的任何帮助。这将需要我们在 CUDA-C 中编写一个主机端*k**ernel launcher*包装函数，我们可以直接调用它，它本身已经编译成一个动态库二进制文件，带有任何必要的 GPU 代码，即 Windows 上的动态链接库（DLL）二进制文件，或 Linux 上的共享对象（so）二进制文件。

当然，我们将从编写 CUDA-C 代码开始，因此打开您最喜欢的文本编辑器并继续。我们将从标准`include`声明开始：

```py
#include <cuda_runtime.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
```

现在我们将直接开始编写内核。注意代码中的`extern "C"`，这将允许我们从外部链接到此功能：

```py
extern "C" __global__ void mandelbrot_ker(float * lattice, float * mandelbrot_graph, int max_iters, float upper_bound_squared, int lattice_size)
{
```

让我们思考一下这将如何工作：我们将使用一个一维数组来表示实部和虚部，称为`lattice`，长度为`lattice_size`。我们将使用它计算形状（`lattice_size`、`lattice_size`的二维 Mandelbrot 图，并将其放入预先分配的数组`mandelbrot_graph`。我们将使用`max_iters`指定检查每个点发散的迭代次数，通过提供其平方值`upper_bound_squared`指定最大上界。（我们马上来看一下使用正方形的动机。）

我们将在一维网格/块结构上启动该内核，每个线程对应于 Mandelbrot 集图形图像中的一个点。然后，我们可以确定对应点的实/虚晶格值，如下所示：

```py
    int tid = blockIdx.x * blockDim.x + threadIdx.x;

    if ( tid < lattice_size*lattice_size )
    {
        int i = tid % lattice_size;
        int j = lattice_size - 1 - (tid / lattice_size);

        float c_re = lattice[i];
        float c_im = lattice[j];
```

让我们谈一分钟。首先，请记住，我们可能需要使用比所需稍多的线程，因此检查线程 ID 是否与`if`语句输出图像中的某个点相对应非常重要。我们还要记住，输出数组`mandelbrot_graph`将存储为一维数组，表示以行格式存储的二维图像，我们将使用`tid`作为索引写入该数组。我们将使用`i`和`j`，以及复杂平面上图形的`x`和`y`坐标。由于晶格是一系列从小到大排序的实值，我们必须颠倒它们的顺序才能得到适当的虚值。另外，请注意，我们将在这里使用普通浮点，而不是使用某种结构或对象来表示复杂值。由于每个复数中都有实部和虚部，因此我们必须在这里使用两个浮点数来存储对应于该线程晶格点的复数（`c_re`和`c_im`。

我们将再设置两个变量来处理发散检查，`z_re`和`z_im`，并在检查发散之前将该线程在图形上的点的初始值设置为`1`：

```py
        float z_re = 0.0f;
        float z_im = 0.0f;

        mandelbrot_graph[tid] = 1;
```

现在我们将检查发散度；如果它在`max_iters`迭代之后发散，我们将该点设置为`0`。否则，它将保留在 1:

```py
        for (int k = 0; k < max_iters; k++)
        {
            float temp;

            temp = z_re*z_re - z_im*z_im + c_re;
            z_im = 2*z_re*z_im + c_im;
            z_re = temp;

            if ( (z_re*z_re + z_im*z_im) > upper_bound_squared )
            {
                mandelbrot_graph[tid] = 0;
                break;
            }
        }
```

在继续之前，让我们先讨论一下这段代码。让我们记住，Mandelbrot 集的每次迭代都是用复数乘法和加法计算的，例如，`z_new = z*z + c`。由于我们没有处理复杂值的类，因此前面的操作正是我们计算`z`的新实值和虚值所需要做的。我们还需要计算绝对值，看看它是否超过某个特定值记住复数的绝对值，*c=x+iy*是用*计算的√（x<sup class="calibre54">2</sup>+y<sup class="calibre54">2</sup>*。实际上，它将为我们节省一些时间来计算上界的平方，然后将其插入内核，因为它将为我们节省计算每次迭代的`z_re*z_re + z_im*z_im`平方根的时间。

我们现在已经基本完成了这个内核，只需关闭`if`语句并从内核返回，我们就完成了：

```py
    }
    return;
}
```

然而，我们还没有完全完成。在 Linux 的情况下，我们只需要编写一个主机端包装函数，在 Windows 的情况下，只需要使用`extern "C"`和`extern "C" __declspec(dllexport)`。（与编译后的 CUDA 内核不同，如果我们希望能够从 Windows 中的 Ctypes 访问主机端函数，那么这个额外的字是必需的。）我们在这个函数中输入的参数将直接对应于进入内核的参数，但这些参数将存储在主机上：

```py
extern "C" __declspec(dllexport) void launch_mandelbrot(float * lattice,  float * mandelbrot_graph, int max_iters, float upper_bound, int lattice_size)
{
```

现在，我们要做的第一个任务是分配足够的内存，用`cudaMalloc`在 GPU 上存储晶格和输出，然后用`cudaMemcpy`将晶格复制到 GPU：

```py
    int num_bytes_lattice = sizeof(float) * lattice_size;
    int num_bytes_graph = sizeof(float)* lattice_size*lattice_size;

    float * d_lattice;
    float * d_mandelbrot_graph;

    cudaMalloc((float **) &d_lattice, num_bytes_lattice);
    cudaMalloc((float **) &d_mandelbrot_graph, num_bytes_graph);

    cudaMemcpy(d_lattice, lattice, num_bytes_lattice, cudaMemcpyHostToDevice);
```

像我们的许多其他内核一样，我们将在一维网格上的大小为 32 的一维块上启动它。我们将计算输出点数的上限值除以 32，以确定网格大小，如下所示：

```py
    int grid_size = (int)  ceil(  ( (double) lattice_size*lattice_size ) / ( (double) 32 ) );
```

现在，我们已经准备好使用传统的 CUDA-C 三个三角形括号来指定网格和块大小，从而启动内核。请注意，我们是如何在此处预先平方上限的：

```py
    mandelbrot_ker <<< grid_size, 32 >>> (d_lattice,  d_mandelbrot_graph, max_iters, upper_bound*upper_bound, lattice_size);
```

现在我们只需要在完成后将输出复制到主机上，然后在适当的数组上调用`cudaFree`。然后我们可以从该函数返回：

```py
    cudaMemcpy(mandelbrot_graph, d_mandelbrot_graph, num_bytes_graph, cudaMemcpyDeviceToHost);    
    cudaFree(d_lattice);
    cudaFree(d_mandelbrot_graph);
}
```

这样，我们就完成了所有需要的 CUDA-C 代码。将其保存到名为`mandelbrot.cu`的文件中，然后继续下一步。

You can also download this file from [https://github.com/btuomanen/handsongpuprogramming/blob/master/10/mandelbrot.cu](https://github.com/btuomanen/handsongpuprogramming/blob/master/10/mandelbrot.cu).

# 编译代码并与 Ctypes 接口

现在，让我们将刚刚编写的代码编译成 DLL 或`.so`二进制文件。这实际上相当轻松：如果您是 Linux 用户，请在命令行中键入以下内容，将此文件编译成`mandelbrot.so`：

```py
nvcc -Xcompiler -fPIC -shared -o mandelbrot.so mandelbrot.cu
```

如果您是 Windows 用户，请在命令行中键入以下内容以将文件编译为`mandelbrot.dll`：

```py
nvcc -shared -o mandelbrot.dll mandelbrot.cu
```

现在我们可以编写 Python 接口了。我们将从适当的导入语句开始，完全排除 PyCUDA，只使用 Ctypes。为了便于使用，我们只需将 Ctypes 中的所有类和函数直接导入默认 Python 名称空间，如下所示：

```py
from __future__ import division
from time import time
import matplotlib
from matplotlib import pyplot as plt
import numpy as np
from ctypes import *
```

让我们使用 Ctypes 为`launch_mandelbrot`主机端函数设置一个接口。首先，我们必须加载已编译的 DLL 或`.so`文件（Linux 用户当然必须将文件名更改为`mandelbrot.so`：

```py
mandel_dll = CDLL('./mandelbrot.dll')
```

Now we can get a reference to `launch_mandelbrot` from the library, like so; we'll call it `mandel_c` for short:

```py
mandel_c = mandel_dll.launch_mandelbrot
```

现在，在我们使用 Ctypes 调用函数之前，我们必须让 Ctypes 知道输入类型是什么。让我们记住，对于`launch_mandelbrot`，输入是`float-pointer`、`float-pointer`、`integer`、`float`和`integer`。我们使用适当的 Ctypes 数据类型（`c_float`、`c_int`）以及 Ctypes`POINTER`类，使用`argtypes`参数进行设置：

```py
mandel_c.argtypes = [POINTER(c_float), POINTER(c_float), c_int, c_float, c_int]
```

现在，让我们编写一个 Python 函数来运行它。我们将使用`breadth`指定方形输出图像的宽度和高度，以及实部和虚部的复数晶格中的最小值和最大值。我们还将指定最大迭代次数以及上限：

```py
def mandelbrot(breadth, low, high, max_iters, upper_bound):
```

现在，我们将使用 NumPy 的`linspace`函数创建晶格阵列，如下所示：

```py
 lattice = np.linspace(low, high, breadth, dtype=np.float32)
```

让我们记住，我们必须将预先分配的浮点数组传递给`launch_mandelbrot`，以输出图的形式获得输出。我们可以通过调用 NumPy 的`empty`命令来设置一个具有适当形状和大小的数组，该数组将在此处充当 C`malloc`调用：

```py
    out = np.empty(shape=(lattice.size,lattice.size), dtype=np.float32)
```

现在，我们准备好计算 Mandelbrot 图。请注意，我们可以使用带有相应类型的`ctypes.data_as`方法将 NumPy 数组传递给 C。完成后，我们可以返回输出；即，二维 NumPy 数组形式的 Mandelbrot 图：

```py
 mandel_c(lattice.ctypes.data_as(POINTER(c_float)), out.ctypes.data_as(POINTER(c_float)), c_int(max_iters), c_float(upper_bound), c_int(lattice.size) ) 
 return out
```

现在，让我们编写一个主要函数，用 Matplotlib 计算、计时和查看 Mandelbrot 图：

```py
if __name__ == '__main__':
    t1 = time()
    mandel = mandelbrot(512,-2,2,256, 2)
    t2 = time()
    mandel_time = t2 - t1
    print 'It took %s seconds to calculate the Mandelbrot graph.' % mandel_time
    plt.figure(1)
    plt.imshow(mandel, extent=(-2, 2, -2, 2))
    plt.show()
```

我们现在将尝试运行此。您应该得到一个与[第 1 章](01.html)，*为什么要进行 GPU 编程中的 Mandelbrot 图完全相同的输出？*和[第三章](03.html)*PyCUDA*入门：

![](assets/0620985f-949b-4f6b-bba4-1be7b0bf7eff.png)

The code for this Python example is also available as the file `mandelbrot_ctypes.py` in the GitHub repository.

# 编译和启动纯 PTX 代码

我们刚刚看到了如何从 Ctypes 调用 pure-C 函数。在某些方面，这可能看起来有点不雅观，因为我们的二进制文件必须同时包含主机代码和编译的 GPU 代码，这可能看起来很麻烦。我们可以只使用纯的、编译过的 GPU 代码，然后将其适当地启动到 GPU 上，而不必每次都编写 C 包装器吗？幸运的是，我们可以。

NVCC 编译器将 CUDA-C 编译成**PTX**（**并行线程执行**），这是一种解释伪汇编语言，兼容 NVIDIA 的各种 GPU 架构。每当您将使用具有 NVCC 的 CUDA 内核的程序编译为可执行 EXE、DLL、`.so`或 ELF 文件时，该文件中都会包含该内核的 PTX 代码。我们还可以直接编译一个扩展名为 PTX 的文件，该文件将只包含编译过的 CUDA.cu 文件中编译过的 GPU 内核。幸运的是，PyCUDA 包含了一个直接从 PTX 加载 CUDA 内核的接口，使我们摆脱了即时编译的束缚，同时还允许我们使用 PyCUDA 的所有其他优秀功能。

现在，让我们把刚才编写的 Mandelbrot 代码编译成一个 PTX 文件；我们不需要对它做任何更改。只需在 Linux 或 Windows 的命令行中键入以下内容：

```py
nvcc -ptx -o mandelbrot.ptx mandelbrot.cu
```

现在让我们修改上一节中的 Python 程序，改为使用 PTX 代码。我们将从导入中删除`ctypes`，并添加相应的 PyCUDA 导入：

```py
from __future__ import division
from time import time
import matplotlib
from matplotlib import pyplot as plt
import numpy as np
import pycuda
from pycuda import gpuarray
import pycuda.autoinit
```

现在让我们使用 PyCUDA 的`module_from_file`函数加载 PTX 文件，如下所示：

```py
mandel_mod = pycuda.driver.module_from_file('./mandelbrot.ptx')
```

现在我们可以通过`get_function`获得对内核的引用，就像 PyCUDA 的`SourceModule`一样：

```py
mandel_ker = mandel_mod.get_function('mandelbrot_ker')
```

现在，我们可以重写 Mandelbrot 函数，用适当的`gpuarray`对象和`typecast`输入来处理这个内核。（我们不会逐行讨论这个问题，因为此时它的功能应该是显而易见的。）：

```py
def mandelbrot(breadth, low, high, max_iters, upper_bound):
    lattice = gpuarray.to_gpu(np.linspace(low, high, breadth, dtype=np.   
    out_gpu = gpuarray.empty(shape=(lattice.size,lattice.size), dtype=np.float32)
    gridsize = int(np.ceil(lattice.size**2 / 32))
    mandel_ker(lattice, out_gpu, np.int32(256), np.float32(upper_bound**2), np.int32(lattice.size), grid=(gridsize, 1, 1), block=(32,1,1))
    out = out_gpu.get()

    return out
```

`main`功能将与上一节完全相同：

```py
if __name__ == '__main__':
    t1 = time()
    mandel = mandelbrot(512,-2,2,256,2)
    t2 = time()
    mandel_time = t2 - t1
    print 'It took %s seconds to calculate the Mandelbrot graph.' % mandel_time
    plt.figure(1)
    plt.imshow(mandel, extent=(-2, 2, -2, 2))
    plt.show()
```

现在，尝试运行此命令以确保输出正确。您可能还注意到，与 Ctypes 版本相比，速度有所提高

This code is also available in the `mandelbrot_ptx.py` file under the "10" directory in this book's GitHub repository.

# 为 CUDA 驱动程序 API 编写包装器

现在，我们将了解如何使用 Ctypes 为一些预打包的二进制 CUDA 库函数编写自己的包装器。特别是，我们将为 CUDA 驱动程序 API 编写包装，这将允许我们执行基本 GPU 使用所需的所有必要操作，包括 GPU 初始化、内存分配/传输/释放、内核启动和上下文创建/同步/销毁。这是一个非常强大的知识；它将允许我们使用我们的 GPU，而无需通过 PyCUDA，也无需编写任何繁琐的主机端 C 函数包装。

现在，我们将编写一个小模块，作为**CUDA 驱动程序 API**的包装库。让我们来讨论一下这意味着什么。驱动程序 API 与**CUDA 运行时 API**稍有不同，技术性稍强，后者是我们在 CUDA-C 的本文中一直在研究的内容。驱动程序 API 设计用于常规 C/C++编译器，而不是 NVCC，具有一些不同的约定，如使用`cuLaunchKernel`函数来启动内核，而不是使用`<<< gridsize, blocksize >>>`括号表示法。这将允许我们直接访问从带有 Ctypes 的 PTX 文件启动内核所需的必要函数。

让我们通过将所有 CType 导入模块的名称空间，然后导入 sys 模块来开始编写这个模块。我们将通过使用`sys.platform`检查系统的操作系统，加载适当的库文件（无论是`nvcuda.dll`还是`libcuda.so`，使我们的模块在 Windows 和 Linux 上都可用，如下所示：

```py
from ctypes import *
import sys
if 'linux' in sys.platform:
 cuda = CDLL('libcuda.so')
elif 'win' in sys.platform:
 cuda = CDLL('nvcuda.dll')
```

我们已经成功地加载了 CUDA 驱动程序 API，现在可以开始为基本 GPU 使用所需的函数编写包装。我们将在接下来的过程中查看每个驱动程序 API 函数的原型，这在编写 Ctypes 包装器时通常是必需的。

The reader is encouraged to look up all of the functions we will be using in this section in the official Nvidia CUDA Driver API Documentation, which is available here: [https://docs.nvidia.com/cuda/cuda-driver-api/](https://docs.nvidia.com/cuda/cuda-driver-api/).

让我们从驱动程序 API 中最基本的函数`cuInit`开始，它将初始化驱动程序 API。它将用于标志的无符号整数作为输入参数，并返回类型为 CUresult 的值，该值实际上只是一个整数值。我们可以这样编写包装器：

```py
cuInit = cuda.cuInit
cuInit.argtypes = [c_uint]
cuInit.restype = int
```

现在让我们开始下一个函数，`cuDeviceCount`，它将告诉我们在计算机上安装了多少 NVIDIA GPU。它接受一个整数指针作为它的单个输入，它实际上是一个通过引用返回的单个整数输出值。返回值是另一个 CUresult 整数，所有函数都将使用 CUresult，这是所有驱动程序 API 函数的错误值的标准化。例如，如果我们看到的任何函数返回一个`0`，这意味着结果是`CUDA_SUCCESS`，而非零结果将始终意味着一个错误或警告：

```py
cuDeviceGetCount = cuda.cuDeviceGetCount
cuDeviceGetCount.argtypes = [POINTER(c_int)]
cuDeviceGetCount.restype = int
```

现在让我们为`cuDeviceGet`编写一个包装器，它将在第一个输入中通过引用返回一个设备句柄。这将对应于第二个输入中给出的顺序 GPU。第一个参数的类型为`CUdevice *`，实际上只是一个整数指针：

```py
cuDeviceGet = cuda.cuDeviceGet
cuDeviceGet.argtypes = [POINTER(c_int), c_int]
cuDeviceGet.restype = int
```

让我们记住，每个 CUDA 会话都需要至少一个 CUDA 上下文，可以将其视为类似于 CPU 上运行的进程。由于这是通过运行时 API 自动处理的，因此在使用设备之前，我们必须在设备上手动创建上下文（使用设备句柄），并且我们必须在 CUDA 会话结束时销毁此上下文

我们可以使用`cuCtxCreate`函数创建 CUDA 上下文，当然，这将创建上下文。让我们看看文档中列出的原型：

```py
 CUresult cuCtxCreate ( CUcontext* pctx, unsigned int flags, CUdevice dev )
```

当然，返回值是`CUresult`。第一个输入是一个指向名为`CUcontext`的类型的指针，实际上它本身就是一个指向 CUDA 内部使用的特定 C 结构的指针。由于我们与 Python 中的`CUcontext`的唯一交互将是保留其值以在其他函数之间传递，因此我们可以将`CUcontext`存储为 C`void *`类型，用于存储任何类型的通用指针地址。由于这实际上是一个指向 CU 上下文的指针（同样，它本身也是指向内部数据结构的指针，这是另一个按引用传递的返回值），因此我们可以将该类型设置为一个普通的`void *`，它是 Ctypes 中的`c_void_p`类型。第二个值是一个无符号整数，而最后一个值是要在其上创建新上下文的设备句柄。请记住，这本身只是一个整数。我们现在准备为`cuCtxCreate`创建包装：

```py
cuCtxCreate = cuda.cuCtxCreate
cuCtxCreate.argtypes = [c_void_p, c_uint, c_int]
cuCtxCreate.restype = int
```

You can always use the `void *` type in C/C++ (`c_void_p` in Ctypes) to point to any arbitrary data or variable—even structures and objects whose definition may not be available.

下一个功能是`cuModuleLoad`，它将为我们加载一个 PTX 模块文件。第一个参数是引用的 CUmodule（同样，我们可以在这里使用一个`c_void_p`，第二个参数是文件名，它将是一个典型的以 null 结尾的 C 字符串—这是一个`char *`，或者 Ctypes 中的`c_char_p`：

```py
cuModuleLoad = cuda.cuModuleLoad
cuModuleLoad.argtypes = [c_void_p, c_char_p]
cuModuleLoad.restype = int
```

下一个函数用于在当前 CUDA 上下文上同步所有启动的操作，称为`cuCtxSynchronize`（不带参数）：

```py
cuCtxSynchronize = cuda.cuCtxSynchronize
cuCtxSynchronize.argtypes = []
cuCtxSynchronize.restype = int
```

下一个函数用于从加载的模块中检索内核函数句柄，以便我们可以将其启动到 GPU 上，这与 PyCUDA 的`get_function`方法完全对应，我们在这一点上已经见过很多次了。文档告诉我们原型是`CUresult cuModuleGetFunction ( CUfunction* hfunc, CUmodule hmod, const char* name )`。我们现在可以编写包装器：

```py
cuModuleGetFunction = cuda.cuModuleGetFunction
 cuModuleGetFunction.argtypes = [c_void_p, c_void_p, c_char_p ]
 cuModuleGetFunction.restype = int
```

现在，让我们编写标准动态内存操作的包装器；这些是必要的，因为我们不会有使用 PyCUDA gpuarray 对象的虚荣心。这些操作实际上与我们以前使用过的 CUDA 运行时操作相同；即，`cudaMalloc`、`cudaMemcpy`和`cudaFree`：

```py
cuMemAlloc = cuda.cuMemAlloc
cuMemAlloc.argtypes = [c_void_p, c_size_t]
cuMemAlloc.restype = int

cuMemcpyHtoD = cuda.cuMemcpyHtoD
cuMemcpyHtoD.argtypes = [c_void_p, c_void_p, c_size_t]
cuMemAlloc.restype = int

cuMemcpyDtoH = cuda.cuMemcpyDtoH
cuMemcpyDtoH.argtypes = [c_void_p, c_void_p, c_size_t]
cuMemcpyDtoH.restype = int

cuMemFree = cuda.cuMemFree
cuMemFree.argtypes = [c_void_p] 
cuMemFree.restype = int
```

现在，我们将为`cuLaunchKernel`函数编写一个包装器。当然，如果我们已经初始化了 CUDA 驱动程序 API，设置了上下文，加载了模块，分配了内存和配置了输入，并从加载的模块中提取了内核函数句柄，那么我们将使用这个函数在 GPU 上启动 CUDA 内核。这个函数比其他函数稍微复杂一些，因此我们将查看原型：

```py
CUresult cuLaunchKernel ( CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream, void** kernelParams, void** extra )  
```

第一个参数是我们想要启动的内核函数的句柄，我们可以将其表示为`c_void_p`。六个`gridDim`和`blockDim`参数用于表示网格和块的尺寸。无符号整数`sharedMemBytes`用于指示内核启动时将为每个块分配多少字节的共享内存。`CUstream hStream`是一个可选参数，我们可以使用它来设置自定义流，或者如果我们希望使用默认流，则将其设置为 NULL（0），我们可以在 Ctypes 中表示为`c_void_p`。最后，使用`kernelParams`和`extra`参数设置内核的输入；这些都有点复杂，所以现在我们也可以将它们表示为`c_void_p`：

```py
cuLaunchKernel = cuda.cuLaunchKernel
cuLaunchKernel.argtypes = [c_void_p, c_uint, c_uint, c_uint, c_uint, c_uint, c_uint, c_uint, c_void_p, c_void_p, c_void_p]
cuLaunchKernel.restype = int
```

现在我们有最后一个函数为`cuCtxDestroy`编写包装器。我们在 CUDA 会话结束时使用它来销毁 GPU 上的上下文。唯一的输入是一个`CUcontext`对象，由`c_void_p`表示：

```py
cuCtxDestroy = cuda.cuCtxDestroy
cuCtxDestroy.argtypes = [c_void_p]
cuCtxDestroy.restype = int
```

让我们将其保存到`cuda_driver.py`文件中。我们现在已经完成了驱动程序 API 包装模块！接下来，我们将了解如何仅使用我们的模块和 Mandelbrot PTX 加载 PTX 模块并启动内核

This example is also available as the `cuda_driver.py` file in this book's GitHub repository.

# 使用 CUDA 驱动程序 API

现在我们将翻译我们的小 Mandelbrot 生成程序，以便我们可以使用我们的包装器库。让我们从适当的导入语句开始；请注意，我们是如何将所有包装器加载到当前名称空间的：

```py
from __future__ import division
from time import time
import matplotlib
from matplotlib import pyplot as plt
import numpy as np
from cuda_driver import *
```

让我们像之前一样，将所有 GPU 代码放入`mandelbrot`函数中。我们将首先用`cuInit`初始化 CUDA 驱动程序 API，然后检查系统上是否至少安装了一个 GPU，否则会引发异常：

```py
def mandelbrot(breadth, low, high, max_iters, upper_bound):
 cuInit(0)
 cnt = c_int(0)
 cuDeviceGetCount(byref(cnt))
 if cnt.value == 0:
  raise Exception('No GPU device found!')
```

注意这里的`byref`：这是 C 编程中引用操作符（`&`的 Ctypes 等价物。现在我们将再次应用这个想法，记住设备句柄和 CUDA 上下文可以用 Ctypes 表示为`c_int`和`c_void_p`：

```py
 cuDevice = c_int(0)
 cuDeviceGet(byref(cuDevice), 0)
 cuContext = c_void_p()
 cuCtxCreate(byref(cuContext), 0, cuDevice)
```

现在我们将加载我们的 PTX 模块，记住将文件名键入带有`c_char_p`的 C 字符串：

```py
 cuModule = c_void_p()
 cuModuleLoad(byref(cuModule), c_char_p('./mandelbrot.ptx'))
```

现在，我们将在主机端设置晶格，以及一个名为`graph`的零的 NumPy 数组，该数组将用于存储主机端的输出。我们还将在 GPU 上为晶格和图形输出分配内存，然后使用`cuMemcpyHtoD`将晶格复制到 GPU：

```py
 lattice = np.linspace(low, high, breadth, dtype=np.float32)
 lattice_c = lattice.ctypes.data_as(POINTER(c_float))
 lattice_gpu = c_void_p(0)
 graph = np.zeros(shape=(lattice.size, lattice.size), dtype=np.float32)
 cuMemAlloc(byref(lattice_gpu), c_size_t(lattice.size*sizeof(c_float)))
 graph_gpu = c_void_p(0)
 cuMemAlloc(byref(graph_gpu), c_size_t(lattice.size**2 * sizeof(c_float)))
 cuMemcpyHtoD(lattice_gpu, lattice_c, c_size_t(lattice.size*sizeof(c_float)))
```

现在我们将通过`cuModuleGetFunction`获得 Mandelbrot 内核的句柄，并设置一些输入：

```py
 mandel_ker = c_void_p(0)
 cuModuleGetFunction(byref(mandel_ker), cuModule, c_char_p('mandelbrot_ker'))
 max_iters = c_int(max_iters)
 upper_bound_squared = c_float(upper_bound**2)
 lattice_size = c_int(lattice.size)
```

下一步要理解有点复杂。在继续之前，我们必须了解参数是如何通过`cuLaunchKernel`传递到 CUDA 内核的。让我们先看看这在 CUDA-C 中是如何工作的。

我们将`kernelParams`中的输入参数表示为`void *`值数组，这些值本身就是指向我们希望插入内核的输入的指针。在我们的 Mandelbrot 内核中，它看起来是这样的：

```py
void * mandel_params [] = {&lattice_gpu, &graph_gpu, &max_iters, &upper_bound_squared, &lattice_size};
```

现在让我们看看如何用 Ctypes 来表达这一点，这一点目前还不明显。首先，让我们按照正确的顺序将所有输入放入 Python 列表中：

```py
mandel_args0 = [lattice_gpu, graph_gpu, max_iters, upper_bound_squared, lattice_size ]
```

现在我们需要指向每一个值的指针，类型转换为`void *`类型。让我们使用 Ctypes 函数`addressof`在这里获取每个 Ctypes 变量的地址（类似于`byref`，只是没有绑定到特定类型），然后将其类型转换为`c_void_p`。我们将这些值存储在另一个列表中：

```py
mandel_args = [c_void_p(addressof(x)) for x in mandel_args0]
```

现在让我们使用 Ctypes 将这个 Python 列表转换为一个`void *`指针数组，如下所示：

```py
 mandel_params = (c_void_p * len(mandel_args))(*mandel_args)
```

我们现在可以像前面一样设置网格的大小，并使用`cuLaunchKernel`使用这组参数启动内核。然后，我们随后同步上下文：

```py
 gridsize = int(np.ceil(lattice.size**2 / 32))
 cuLaunchKernel(mandel_ker, gridsize, 1, 1, 32, 1, 1, 10000, None, mandel_params, None)
 cuCtxSynchronize()
```

现在，我们将使用带有 NumPy`array.ctypes.data`成员的`cuMemcpyDtoH`将数据从 GPU 复制到 NumPy 数组中，这是一个 C 指针，允许我们作为堆内存块直接从 C 访问数组。我们将使用 Ctypes typecast 函数`cast`将其类型转换为`c_void_p`：

```py
 cuMemcpyDtoH( cast(graph.ctypes.data, c_void_p), graph_gpu,  c_size_t(lattice.size**2 *sizeof(c_float)))
```

我们现在完成了！让我们释放在 GPU 上分配的阵列，并通过销毁当前上下文结束 GPU 会话。然后，我们将图形 NumPy 数组返回给调用函数：

```py
 cuMemFree(lattice_gpu)
 cuMemFree(graph_gpu)
 cuCtxDestroy(cuContext)
 return graph
```

现在我们可以像以前一样设置我们的`main`功能：

```py
if __name__ == '__main__':
 t1 = time()
 mandel = mandelbrot(512,-2,2,256, 2)
 t2 = time()
 mandel_time = t2 - t1
 print 'It took %s seconds to calculate the Mandelbrot graph.' % mandel_time

 fig = plt.figure(1)
 plt.imshow(mandel, extent=(-2, 2, -2, 2))
 plt.show()
```

现在试着运行这个函数，以确保它产生与我们刚才编写的其他 Mandelbrot 程序相同的输出

祝贺您刚刚编写了一个到低级 CUDA 驱动程序 API 的直接接口，并成功地用它启动了一个内核！ 

This program is also available as the `mandelbrot_driver.py` file under the directory in this book's GitHub repository.

# 总结

本章开始时，我们简要概述了 PythoncTypes 库，该库用于直接与已编译的二进制代码交互，特别是用 C/C++编写的动态库。然后，我们研究了如何使用 CUDA-C 编写一个基于 C 的包装器来启动 CUDA 内核，然后使用它通过使用 Ctypes 为该函数编写一个接口来间接地从 Python 启动 CUDA 内核。然后，我们学习了如何将 CUDA 内核编译成 PTX 模块二进制文件，该二进制文件可以被认为是 DLL，但具有 CUDA 内核函数，并了解了如何加载 PTX 文件和使用 PyCUDA 启动预编译内核。最后，我们为 CUDA 驱动程序 API 编写了一组 Ctypes 包装器，并了解了如何使用这些包装器执行基本的 GPU 操作，包括从 PTX 文件启动预编译内核到 GPU。

现在，我们将进入本书最具技术性的章节：第 11 章，CUDA 中的*性能优化。在本章中，我们将了解 NVIDIA GPU 的一些技术细节，这些细节将帮助我们提高应用程序的性能水平。*

# 问题

1.  假设您使用`nvcc`将包含主机和内核代码的单个`.cu`文件编译成 EXE 文件，也可以编译成 PTX 文件。哪个文件将包含主机功能，哪个文件将包含 GPU 代码？
2.  如果我们使用 CUDA 驱动程序 API，为什么要破坏上下文？
3.  在本章的开头，当我们第一次看到如何使用 Ctypes 时，请注意，在调用`printf`之前，我们必须将浮点值 3.14 类型转换为 Ctypes`c_double`对象。然而，在本章中，我们可以看到许多不使用 Ctypes 的工作案例。你为什么认为`printf`在这里是个例外？
4.  假设您希望向 Python CUDA 驱动程序接口模块添加功能以支持 CUDA 流。如何在 Ctypes 中表示单个流对象？
5.  为什么我们在`mandelbrot.cu`中的函数中使用`extern "C"`？
6.  再看一次`mandelbrot_driver.py`。为什么我们*不*在 GPU 内存分配和主机/GPU 内存传输之后，并且仅在单内核调用之后，才使用`cuCtxSynchronize`功能？