# 探索 Windows 法医人工制品配方-第一部分

本章将介绍以下配方：

*   一个人的垃圾是法医的宝藏
*   棘手的局面
*   读取注册表
*   收集用户活动
*   缺失的环节
*   到处寻找

# 介绍

Windows 长期以来一直是 PC 市场上首选的操作系统。事实上，Windows 在访问政府网站的用户中约占 47%，而第二流行的个人电脑操作系统 macOS 仅占 8.5%。没有理由怀疑这种情况会很快改变，特别是在 Windows10 受到热烈欢迎的情况下。因此，未来的调查极有可能继续需要对 Windows 工件进行分析。

本章介绍了许多类型的工件，以及如何使用 Python 直接从法医证据容器中使用各种第一方和第三方库来解释它们。我们将利用我们在[第 8 章](08.html#75QNI0-260f9401d2714cb9ab693c4692308abe)中开发的框架*与法医证据容器配方*一起工作，直接从法医采集中处理这些人工制品。通过这种方式，我们可以向代码提供捕获的原始或 EWF 图像，而不必担心在处理数据之前提取所需文件或装载图像的过程。具体而言，我们将涵盖：

*   解读`$I`文件，了解更多关于发送到回收站的文件的信息
*   在 Windows 7 系统上从便笺中读取内容和元数据
*   从注册表提取值以了解操作系统版本和其他配置详细信息
*   显示与搜索、键入的路径和运行命令相关的用户活动
*   解析 LNK 文件以了解历史和最近的文件访问
*   检查`Windows.edb`以获取有关索引文件、文件夹和消息的信息

To view more interesting metrics, visit [https://analytics.usa.gov/](https://analytics.usa.gov/).

请访问[www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)下载本章的代码包。

# 一个人的垃圾是法医的宝藏

难度：中等

Python 版本：2.7

操作系统：Linux

虽然这可能不是确切的说法，但对驻留在回收站中的已删除文件进行法医检查是大多数调查中的一个重要步骤。非技术保管人可能不理解发送到回收站的这些文件仍然存在，并且我们可以了解很多关于原始文件的信息，例如原始文件路径和发送到回收站的时间。虽然不同版本的 Windows 的具体工件有所不同，但此配方主要关注回收站的`$I`和`$R`文件的 Windows 7 版本。

# 开始

此配方需要安装三个第三方模块才能工作：`pytsk3`、`pyewf`和`unicodecsv`。*有关安装`pytsk3`和`pyewf`模块*的详细说明，请参阅[第 8 章](08.html#75QNI0-260f9401d2714cb9ab693c4692308abe)，使用法医证据容器配方*。*此脚本中使用的所有其他库都存在于 Python 的标准库中

因为我们在 Python2.x 中开发这些方法，所以可能会遇到 Unicode 编码和解码错误。为了说明这一点，我们使用`unicodecsv`库编写本章中的所有 CSV 输出。与 Python2.x 的标准`csv`模块不同，这个第三方模块负责 Unicode 支持，并将在这里大量使用。通常我们可以使用`pip`安装`unicodecsv`：

```py
pip install unicodecsv==0.14.1
```

To learn more about the `unicodecsv` library, visit [https://github.com/jdunck/python-unicodecsv](https://github.com/jdunck/python-unicodecsv).

除此之外，我们将继续使用从[第 8 章](https://cdp.packtpub.com/python_digital_forensics_cookbook/wp-admin/post.php?post=260&action=edit#post_218)、*开发的`pytskutil`模块，使用法医证据容器配方*、*、*与法医采集进行交互。本模块与我们之前编写的内容基本相似，只是做了一些小改动，以更好地满足我们的目的。您可以通过导航到代码包中的实用程序目录来查看代码。

# 怎么做。。。

要解析 Windows 7 机器上的`$I`和`$R`文件，我们需要：

1.  通过证据文件中的`$Recycle.bin`文件夹递归，选择以`$I`开头的所有文件。
2.  读取文件的内容并分析可用的元数据结构。
3.  搜索相关的`$R`文件并检查它是否是文件或文件夹。
4.  将结果写入 CSV 文件以供审查。

# 它是如何工作的。。。

我们导入了`argparse`、`datetime`、`os`和`struct`内置库，以帮助运行脚本和解释这些文件中的二进制数据。我们还引入了用于处理证据文件、读取内容以及遍历文件夹和文件的侦探工具包实用程序。最后，我们导入`unicodecsv`库以帮助编写 CSV 报告：

```py
from __future__ import print_function
from argparse import ArgumentParser
import datetime
import os
import struct

from utility.pytskutil import TSKUtil
import unicodecsv as csv
```

此配方的命令行处理程序采用三个位置参数，`EVIDENCE_FILE`、`IMAGE_TYPE`和`CSV_REPORT`，分别表示证据文件的路径、证据文件的类型以及 CSV 报告的所需输出路径。这三个参数被传递给`main()`函数。

```py
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=__description__,
        epilog="Developed by {} on {}".format(
            ", ".join(__authors__), __date__)
    )
    parser.add_argument('EVIDENCE_FILE', help="Path to evidence file")
    parser.add_argument('IMAGE_TYPE', help="Evidence file format",
                        choices=('ewf', 'raw'))
    parser.add_argument('CSV_REPORT', help="Path to CSV report")
    args = parser.parse_args()
    main(args.EVIDENCE_FILE, args.IMAGE_TYPE, args.CSV_REPORT)
```

`main()`函数处理与证据文件的必要交互，以识别并提供任何`$I`文件进行处理。要访问证据文件，必须提供容器的路径和图像类型。这将启动`TSKUtil`实例，我们使用该实例搜索图像中的文件和文件夹。为了找到`$I`文件，我们在`tsk_util`实例上调用`recurse_files()`方法，指定要查找的文件名模式、`path`开始搜索，以及用于查找文件名的字符串`logic`。`logic`关键字参数接受以下与字符串操作相对应的值：`startswith`、`endswith`、`contains`和`equals`。这些命令指示用于在扫描的文件和文件夹名称中搜索我们的`$I`模式的字符串操作。

如果找到任何`$I`文件，我们将此列表与`tsk_util`对象一起传递给`process_dollar_i()`函数。处理完后，我们使用`write_csv()`方法将提取的元数据写入 CSV 报告：

```py
def main(evidence, image_type, report_file):
    tsk_util = TSKUtil(evidence, image_type)

    dollar_i_files = tsk_util.recurse_files("$I", path='/$Recycle.bin',
                                            logic="startswith")

    if dollar_i_files is not None:
        processed_files = process_dollar_i(tsk_util, dollar_i_files)

        write_csv(report_file,
                  ['file_path', 'file_size', 'deleted_time',
                   'dollar_i_file', 'dollar_r_file', 'is_directory'],
                  processed_files)
    else:
        print("No $I files found")
```

`process_dollar_i()`函数接受`tsk_util`对象和发现的`$I`文件列表作为输入。我们遍历这个列表并检查每个文件。`dollar_i_files`列表中的每个元素本身就是一个元组列表，其中每个元组元素（按顺序）包含文件名、相对路径、访问文件内容的句柄和文件系统标识符。有了这些可用属性，我们将调用`read_dollar_i()`函数并为其提供第三个元组，即文件对象句柄。如果这是一个有效的`$I`文件，则此方法返回从原始文件提取的元数据字典，否则返回`None`。如果文件有效，我们通过将`$I`文件的文件路径添加到`file_attribs`字典继续处理：

```py
def process_dollar_i(tsk_util, dollar_i_files):
    processed_files = []
    for dollar_i in dollar_i_files:
        # Interpret file metadata
        file_attribs = read_dollar_i(dollar_i[2])
        if file_attribs is None:
            continue # Invalid $I file
        file_attribs['dollar_i_file'] = os.path.join(
            '/$Recycle.bin', dollar_i[1][1:])
```

接下来，我们在图像中搜索相关的`$R`文件。为此，我们将基本路径连接到`$I`文件（包括`$Recycle.bin`和`SID`文件夹），以减少搜索相应`$R`文件所需的时间。在 Windows 7 上，`$I`和`$R`文件具有相似的文件名，其中前两个字母分别为`$I`和`$R`，后跟一个共享标识符。通过在搜索中使用该标识符并指定我们希望找到的`$R`文件的特定文件夹，我们降低了误报的可能性。使用这些模式，我们使用`startswith`逻辑再次查询我们的证据文件：

```py
        # Get the $R file
        recycle_file_path = os.path.join(
            '/$Recycle.bin',
            dollar_i[1].rsplit("/", 1)[0][1:]
        )
        dollar_r_files = tsk_util.recurse_files(
            "$R" + dollar_i[0][2:],
            path=recycle_file_path, logic="startswith"
        )
```

如果搜索`$R`文件失败，我们将尝试查询具有相同信息的目录。如果此查询也不成功，我们会附加字典值，说明找不到`$R`文件，并且我们不确定它是文件还是目录。但是，如果我们找到了匹配的目录，我们会记录目录的路径，并将`is_directory`属性设置为`True`：

```py
        if dollar_r_files is None:
            dollar_r_dir = os.path.join(recycle_file_path,
                                        "$R" + dollar_i[0][2:])
            dollar_r_dirs = tsk_util.query_directory(dollar_r_dir)
            if dollar_r_dirs is None:
                file_attribs['dollar_r_file'] = "Not Found"
                file_attribs['is_directory'] = 'Unknown'
            else:
                file_attribs['dollar_r_file'] = dollar_r_dir
                file_attribs['is_directory'] = True
```

如果对`$R`文件的搜索返回一个或多个点击，我们将使用列表理解创建一个匹配文件列表，以存储在 CSV 中，用分号分隔，并将`is_directory`属性标记为`False`。

```py
        else:
            dollar_r = [os.path.join(recycle_file_path, r[1][1:])
                        for r in dollar_r_files]
            file_attribs['dollar_r_file'] = ";".join(dollar_r)
            file_attribs['is_directory'] = False
```

在退出循环之前，我们将`file_attribs`字典附加到`processed_files`列表中，该列表存储所有`$I`处理过的字典。此词典列表将返回到`main()`函数，并在报告过程中使用。

```py
        processed_files.append(file_attribs)
    return processed_files
```

让我们简要地看一下`read_dollar_i()`方法，它用于解析带有`struct`的二进制文件中的元数据。我们首先检查文件头，使用 Sleuth Kit 的`read_random()`方法读取签名的前八个字节。如果签名不匹配，我们返回`None`以警告`$I`未通过验证，并且是无效的文件格式。

```py
def read_dollar_i(file_obj):
    if file_obj.read_random(0, 8) != '\x01\x00\x00\x00\x00\x00\x00\x00':
        return None # Invalid file
```

如果我们检测到一个有效的文件，我们将继续从`$I`文件中读取并解压缩值。第一个是文件大小属性，位于字节偏移量`8`处，长度为`8`字节。我们用`struct`解包并将整数存储在一个临时变量中。下一个属性，删除时间，存储在字节偏移量`16`和`8`字节长处。这是一个 Windows`FILETIME`对象，我们将借用一些旧代码，稍后将其处理为人类可读的时间戳。最后一个属性是前一个文件路径，我们从字节`24`读取到文件末尾：

```py
    raw_file_size = struct.unpack('<q', file_obj.read_random(8, 8))
    raw_deleted_time = struct.unpack('<q', file_obj.read_random(16, 8))
    raw_file_path = file_obj.read_random(24, 520)
```

通过提取这些值，我们将整数解释为人类可读的值。我们使用`sizeof_fmt()`函数将文件大小整数转换为人类可读的大小，其中包含大小前缀，如 MB 或 GB。接下来，我们使用[第 7 章](07.html#6A5N80-260f9401d2714cb9ab693c4692308abe)、*基于日志的工件配方*中的日期解析配方中的逻辑解释时间戳（在调整函数以仅处理整数之后）。最后，我们将路径解码为 UTF-16 并删除空字节值。这些细化的详细信息随后作为字典返回给调用函数：

```py
    file_size = sizeof_fmt(raw_file_size[0])
    deleted_time = parse_windows_filetime(raw_deleted_time[0])
    file_path = raw_file_path.decode("utf16").strip("\x00")
    return {'file_size': file_size, 'file_path': file_path,
            'deleted_time': deleted_time}
```

我们的`sizeof_fmt()`功能是从[StackOverflow.com](https://stackoverflow.com/)中借用的，该网站提供了许多编程问题的解决方案。虽然我们可以起草自己的代码，但该代码的格式符合我们的目的。它取整数`num`并遍历列出的单位后缀。如果数字小于`1024`，则将数字、单位和后缀连接成字符串并返回；否则，数字除以`1024`并运行下一次迭代。如果该数字大于 zettabyte，它将以 yottabytes 为单位返回信息。为了你的缘故，我们希望这个数字永远不会那么大。

```py
def sizeof_fmt(num, suffix='B'):
    # From https://stackoverflow.com/a/1094933/3194812
    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:
        if abs(num) < 1024.0:
            return "%3.1f%s%s" % (num, unit, suffix)
        num /= 1024.0
    return "%.1f%s%s" % (num, 'Yi', suffix)
```

我们的下一个支持函数是`parse_windows_filetime()`，改编自[第 7 章](07.html#6A5N80-260f9401d2714cb9ab693c4692308abe)*基于日志的工件配方*中先前的日期解析配方。我们借用逻辑并压缩代码，仅将整数解释为格式化日期，然后返回给调用函数。通用函数，就像我们刚才讨论的两个，可以方便地保存在您的库中，因为您永远不知道何时可能需要这种逻辑。

```py
def parse_windows_filetime(date_value):
    microseconds = float(date_value) / 10
    ts = datetime.datetime(1601, 1, 1) + datetime.timedelta(
        microseconds=microseconds)
    return ts.strftime('%Y-%m-%d %H:%M:%S.%f')
```

最后，我们准备将处理后的结果写入 CSV 文件。毫无疑问，正如您所期待的，此函数与我们的所有其他 CSV 函数类似。唯一的区别是它在引擎盖下使用`unicodecsv`库，虽然这里使用的方法和函数名是相同的：

```py
def write_csv(outfile, fieldnames, data):
    with open(outfile, 'wb') as open_outfile:
        csvfile = csv.DictWriter(open_outfile, fieldnames)
        csvfile.writeheader()
        csvfile.writerows(data)
```

在以下两个屏幕截图中，我们可以看到此配方从`$I`和`$R`文件中提取的数据类型示例：

![](../images/00096.jpeg)

![](../images/00097.jpeg)

# 棘手的局面

难度：中等

Python 版本：2.7

操作系统：Linux

计算机已经取代了笔和纸。我们已经将许多流程和习惯转移到这些机器上，其中一个仅限于纸张，包括记笔记和制作列表。一个复制现实世界习惯的功能是 Windows 便笺。这些便笺允许持久性便笺在桌面上浮动，并提供颜色、字体等选项。这个配方将允许我们探索这些便笺，并将它们添加到我们的调查工作流程中。

# 开始

此配方需要安装四个第三方模块才能工作：`olefile`、`pytsk3`、`pyewf`和`unicodecsv`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第 8 章](08.html#75QNI0-260f9401d2714cb9ab693c4692308abe)、*使用法医证据容器**配方*。同样，有关安装`unicodecsv`的详细信息，请参阅*一人垃圾是法医宝藏*配方中的*入门*部分。此脚本中使用的所有其他库都存在于 Python 的标准库中。

Windows 便笺文件存储为`OLE`文件。因此，我们将利用`olefile`库与 Windows 便笺交互并从中提取数据。`olefile`库可安装`pip`：

```py
pip install olefile==0.44
```

To learn more about the `olefile` library, visit [https://olefile.readthedocs.io/en/latest/index.html](https://olefile.readthedocs.io/en/latest/index.html).

# 怎么做。。。

要正确形成此配方，我们需要采取以下步骤：

1.  打开证据文件，在用户配置文件中查找所有`StickyNote.snt`文件。
2.  解析 OLE 流中的元数据和内容。
3.  将 RTF 内容写入文件。
4.  创建元数据的 CSV 报告。

# 它是如何工作的。。。

与其他脚本一样，该脚本以执行所需库的 import 语句开始。这里的两个新库是`olefile`，正如我们所讨论的，它解析 Windows Sticky Note OLE streams 和`StringIO`，一个内置库，用于将字符串数据解释为类似文件的对象。此库将用于将`pytsk`文件对象转换为`olefile`库可以解释的流：

```py
from __future__ import print_function
from argparse import ArgumentParser
import unicodecsv as csv
import os
import StringIO

from utility.pytskutil import TSKUtil
import olefile
```

我们指定一个全局变量`REPORT_COLS`，它表示报表列。这些静态列将在多个函数中使用。

```py
REPORT_COLS = ['note_id', 'created', 'modified', 'note_text', 'note_file']
```

此配方的命令行处理程序采用三个位置参数，`EVIDENCE_FILE`、`IMAGE_TYPE`和`REPORT_FOLDER`，分别表示证据文件的路径、证据文件的类型和所需的输出目录路径。这与前面的配方类似，除了`REPORT_FOLDER`之外，这是一个目录，我们将把便笺 RTF 文件写入其中：

```py
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=__description__,
        epilog="Developed by {} on {}".format(
            ", ".join(__authors__), __date__)
    )
    parser.add_argument('EVIDENCE_FILE', help="Path to evidence file")
    parser.add_argument('IMAGE_TYPE', help="Evidence file format",
                        choices=('ewf', 'raw'))
    parser.add_argument('REPORT_FOLDER', help="Path to report folder")
    args = parser.parse_args()
    main(args.EVIDENCE_FILE, args.IMAGE_TYPE, args.REPORT_FOLDER)
```

我们的主要功能与上一个类似，通过处理证据文件并搜索我们试图解析的文件来启动。在本例中，我们正在查找`StickyNotes.snt`文件，该文件位于每个用户的`AppData`目录中。因此，我们将搜索限制在`/Users`文件夹，并查找与确切名称匹配的文件：

```py
def main(evidence, image_type, report_folder):
    tsk_util = TSKUtil(evidence, image_type)
    note_files = tsk_util.recurse_files('StickyNotes.snt', '/Users',
                                        'equals')
```

然后，我们迭代生成的文件，分离出用户的主目录名，并设置`olefile`库处理所需的类似文件的对象。接下来，我们调用`parse_snt_file()`函数来处理该文件，并返回一个要迭代的结果列表。此时，如果`note_data`不是`None`，我们将使用`write_note_rtf()`方法写入 RTF 文件。此外，我们将处理后的数据从`prep_note_report()`添加到`report_details`列表中。一旦`for`循环完成，我们将使用`write_csv()`方法编写 CSV 报告，方法是提供报告名称、报告列以及我们构建的便笺信息列表。

```py
    report_details = []
    for note_file in note_files:
        user_dir = note_file[1].split("/")[1]
        file_like_obj = create_file_like_obj(note_file[2])
        note_data = parse_snt_file(file_like_obj)
        if note_data is None:
            continue
        write_note_rtf(note_data, os.path.join(report_folder, user_dir))
        report_details += prep_note_report(note_data, REPORT_COLS,
                                           "/Users" + note_file[1])
    write_csv(os.path.join(report_folder, 'sticky_notes.csv'), REPORT_COLS,
              report_details)
```

`create_file_like_obj()`函数接受我们的`pytsk`文件对象并读取文件的大小。在`read_random()`函数中使用此大小将整个便笺内容读入内存。我们将`file_content`输入`StringIO()`类，将其转换为`olefile`库在返回父函数之前可以读取的类似文件的对象：

```py
def create_file_like_obj(note_file):
    file_size = note_file.info.meta.size
    file_content = note_file.read_random(0, file_size)
    return StringIO.StringIO(file_content)
```

`parse_snt_file()`函数接受类似文件的对象作为输入，用于读取和解释便笺文件。我们首先验证类文件对象是否为 OLE 文件，如果不是，则返回`None`。如果是，我们使用`OleFileIO()`方法打开类似文件的对象。这提供了一个流列表，允许我们遍历每个便笺的每个元素。当我们在列表上迭代时，我们检查流是否包含三个破折号，因为这表明流包含便笺的唯一标识符。此文件可以包含一个或多个便笺，每个便笺由唯一的 ID 标识。根据流的第一个索引中元素的值，将便笺数据直接读取为 RTF 数据或 UTF-16 编码数据。

我们还分别使用`getctime()`和`getmtime()`函数从流中读取创建和修改的信息。接下来，我们将便签 RTF 或 UTF-16 编码的数据提取到`content`变量。注意，我们必须在存储 UTF-16 编码数据之前对其进行解码。如果有内容要保存，我们将其添加到`note`字典中，并继续处理所有剩余的流。处理完所有流后，`note`字典返回父函数：

```py
def parse_snt_file(snt_file):
    if not olefile.isOleFile(snt_file):
        print("This is not an OLE file")
        return None
    ole = olefile.OleFileIO(snt_file)
    note = {}
    for stream in ole.listdir():
        if stream[0].count("-") == 3:
            if stream[0] not in note:
                note[stream[0]] = {
                    # Read timestamps
                    "created": ole.getctime(stream[0]),
                    "modified": ole.getmtime(stream[0])
                }

            content = None
            if stream[1] == '0':
                # Parse RTF text
                content = ole.openstream(stream).read()
            elif stream[1] == '3':
                # Parse UTF text
                content = ole.openstream(stream).read().decode("utf-16")

            if content:
                note[stream[0]][stream[1]] = content

    return note
```

为了创建 RTF 文件，我们将注释数据字典传递给`write_note_rtf()`函数。如果报表文件夹不存在，我们使用`os`库创建它。此时，我们遍历`note_data`字典，将`note_id`键与`stream_data`值分开。`note_id`用于在打开之前创建输出 RTF 文件名。

然后，在关闭输出 RTF 文件和处理下一个便笺之前，将存储在零流中的数据写入输出 RTF 文件：

```py
def write_note_rtf(note_data, report_folder):
    if not os.path.exists(report_folder):
        os.makedirs(report_folder)
    for note_id, stream_data in note_data.items():
        fname = os.path.join(report_folder, note_id + ".rtf")
        with open(fname, 'w') as open_file:
            open_file.write(stream_data['0'])
```

写了便笺的内容后，我们现在转到 CSV 报告本身，`prep_note_report()`函数对该报告的处理略有不同。这会将嵌套字典转换为一个更适合 CSV 电子表格的字典平面列表。我们通过包含`note_id`键并使用全局`REPORT_COLS`列表中指定的键命名字段来将其展平。

```py
def prep_note_report(note_data, report_cols, note_file):
    report_details = []
    for note_id, stream_data in note_data.items():
        report_details.append({
            "note_id": note_id,
            "created": stream_data['created'],
            "modified": stream_data['modified'],
            "note_text": stream_data['3'].strip("\x00"),
            "note_file": note_file
        })
    return report_details
```

最后，在`write_csv()`方法中，我们创建一个`csv.Dictwriter`对象来创建便笺数据的概览报告。此 CSV 编写器还使用`unicodecsv`库，并将字典列表写入文件，使用`REPORT_COLS`列列表作为`fieldnames`。

```py
def write_csv(outfile, fieldnames, data):
    with open(outfile, 'wb') as open_outfile:
        csvfile = csv.DictWriter(open_outfile, fieldnames)
        csvfile.writeheader()
        csvfile.writerows(data)
```

然后，我们可以查看输出，因为我们有一个包含导出的便笺和报告的新目录：

![](../images/00098.jpeg)

打开报告，我们可以查看便笺元数据并收集一些内部内容，尽管大多数电子表格查看者难以理解非 ASCII 字符：

![](../images/00099.jpeg)

最后，我们可以打开输出 RTF 文件并查看原始内容：

![](../images/00100.jpeg)

# 读取注册表

难度：中等

Python 版本：2.7

操作系统：Linux

Windows 注册表包含许多与操作系统配置、用户活动、软件安装和使用等相关的重要细节。由于这些文件所包含的工件数量及其与 Windows 系统的相关性，因此通常会对其进行严格的检查和研究。通过解析注册表文件，我们可以访问可以显示基本操作系统信息的键和值、对文件夹和文件的访问、应用程序使用情况、USB 设备等。在本配方中，我们着重于访问来自`SYSTEM`和`SOFTWARE`蜂箱的公共基线信息。

# 开始

此配方需要安装三个第三方模块才能工作：`pytsk3`、`pyewf`和`Registry`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第 8 章](08.html#75QNI0-260f9401d2714cb9ab693c4692308abe)、*使用法医证据容器**配方、*。此脚本中使用的所有其他库都存在于 Python 的标准库中。

在此配方中，我们使用`Registry`模块以面向对象的方式与注册表配置单元进行交互。关键是，此模块可用于与外部和独立注册表文件交互。`Registry`模块可安装`pip`：

```py
pip install python-registry==1.0.4
```

To learn more about the `Registry` library, visit [https://github.com/williballenthin/python-registry](https://github.com/williballenthin/python-registry).

# 怎么做。。。

要构建注册表系统概述脚本，我们需要：

1.  按名称和路径查找要处理的注册表配置单元。
2.  使用`StringIO`和`Registry`模块打开这些文件。
3.  处理每个配置单元，将解析后的值打印到控制台进行解释。

# 它是如何工作的。。。

进口与本章中的其他配方重叠。这些模块允许我们处理参数解析、日期操作、将文件读入`Registry`库的内存，以及解包和解释从注册表值提取的二进制数据。我们还导入了`TSKUtil()`类和`Registry`模块来处理注册表文件。

```py
from __future__ import print_function
from argparse import ArgumentParser
import datetime
import StringIO
import struct

from utility.pytskutil import TSKUtil
from Registry import Registry
```

此配方的命令行处理程序采用两个位置参数`EVIDENCE_FILE`和`IMAGE_TYPE`，分别表示证据文件的路径和证据文件的类型：

```py
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=__description__,
        epilog="Developed by {} on {}".format(
            ", ".join(__authors__), __date__)
    )
    parser.add_argument('EVIDENCE_FILE', help="Path to evidence file")
    parser.add_argument('IMAGE_TYPE', help="Evidence file format",
                        choices=('ewf', 'raw'))
    args = parser.parse_args()
    main(args.EVIDENCE_FILE, args.IMAGE_TYPE)
```

`main()`函数首先从证据中创建一个`TSKUtil`对象，然后在`/Windows/System32/config`文件夹中搜索`SYSTEM`和`SOFTWARE`蜂箱。我们使用`open_file_as_reg()`函数创建这些蜂巢的`Registry()`类实例，然后将它们传递给各自的处理函数。

```py
def main(evidence, image_type):
    tsk_util = TSKUtil(evidence, image_type)
    tsk_system_hive = tsk_util.recurse_files(
        'system', '/Windows/system32/config', 'equals')
    tsk_software_hive = tsk_util.recurse_files(
        'software', '/Windows/system32/config', 'equals')

    system_hive = open_file_as_reg(tsk_system_hive[0][2])
    software_hive = open_file_as_reg(tsk_software_hive[0][2])

    process_system_hive(system_hive)
    process_software_hive(software_hive)
```

要打开注册表文件，我们需要从`pytsk`元数据中收集文件的大小，并将整个文件（从字节 0 到文件末尾）读取到一个变量中。然后，我们将此变量提供给一个`StringIO()`实例，该实例允许我们使用`Registry()`类打开类似文件的对象。我们将`Registry`类实例返回给调用函数进行进一步处理：

```py
def open_file_as_reg(reg_file):
    file_size = reg_file.info.meta.size
    file_content = reg_file.read_random(0, file_size)
    file_like_obj = StringIO.StringIO(file_content)
    return Registry.Registry(file_like_obj)
```

让我们从`SYSTEM`蜂箱处理开始。此配置单元将其大部分信息保存在控制集中。`SYSTEM`蜂箱通常有两个或两个以上的控制集，作为存储配置的备份系统。为简单起见，我们将只读取当前控制集。为了识别当前的控制集，我们使用`root`键在蜂箱中找到了立足点，并使用`find_key()`方法获得`Select`键。在这个键中，我们读取`Current`值，使用`value()`方法选择它，并使用`value`对象上的`value()`方法显示值的内容。虽然方法命名有点模棱两可，但键中的值是命名的，因此我们首先需要按名称选择它们，然后再调用它们所包含的内容。使用此信息，我们选择当前控件集键，为当前控件集传递一个适当的填充整数（例如`ControlSet0001`。此对象将在函数的其余部分用于导航到特定的`subkeys`和`values`：

```py
def process_system_hive(hive):
    root = hive.root()
    current_control_set = root.find_key("Select").value("Current").value()
    control_set = root.find_key("ControlSet{:03d}".format(
        current_control_set))
```

我们将从`SYSTEM`蜂箱中提取的第一条信息是关机时间。我们从当前控制集中读取`Control\Windows\ShutdownTime`值，并将十六进制值传递到`struct`以将其转换为`64-bit`整数。然后，我们将这个整数提供给 Windows`FILETIME`解析器，以获得一个人类可读的日期字符串，并将其打印到控制台。

```py
    raw_shutdown_time = struct.unpack(
        '<Q', control_set.find_key("Control").find_key("Windows").value(
            "ShutdownTime").value()
    )
    shutdown_time = parse_windows_filetime(raw_shutdown_time[0])
    print("Last Shutdown Time: {}".format(shutdown_time))
```

接下来，我们将确定机器的时区信息。这在`Control\TimeZoneInformation\TimeZoneKeyName`值范围内。这将返回一个字符串值，我们可以直接打印到控制台：

```py
    time_zone = control_set.find_key("Control").find_key(
        "TimeZoneInformation").value("TimeZoneKeyName").value()
    print("Machine Time Zone: {}".format(time_zone))
```

然后，我们收集机器的主机名。这在`ComputerName`值的`Control\ComputerName\ComputerName`键下找到。提取的值是可以打印到控制台的字符串：

```py
    computer_name = control_set.find_key(
        "Control").find_key("ComputerName").find_key(
            "ComputerName").value("ComputerName").value()
    print("Machine Name: {}".format(computer_name))
```

到目前为止很容易，对吧？最后，对于`System`配置单元，我们解析关于上次访问时间戳配置的信息。此`registry`键确定是否保留 NTFS 卷的上次访问时间戳，并且通常在系统上默认禁用。为了确认这一点，我们在`Control\FileSystem`键中查找`NtfsDisableLastAccessUpdate`值，看看它是否等于`1`。如果是，则在打印到控制台之前，不会维护最后访问时间戳并将其标记为禁用。请注意一行`if-else`语句，虽然它可能更难阅读，但它确实有其用途：

```py
    last_access = control_set.find_key("Control").find_key(
        "FileSystem").value("NtfsDisableLastAccessUpdate").value()
    last_access = "Disabled" if last_access == 1 else "enabled"
    print("Last Access Updates: {}".format(last_access))
```

我们的 Windows`FILETIME`解析器借用了我们以前的日期解析方法中的逻辑，接受一个整数，并将其转换为人类可读的日期字符串。我们还从相同的日期解析配方中借用了`Unix`纪元日期解析器的逻辑，并将使用它来解释`Software`蜂巢中的日期。

```py
def parse_windows_filetime(date_value):
    microseconds = float(date_value) / 10
    ts = datetime.datetime(1601, 1, 1) + datetime.timedelta(
        microseconds=microseconds)
    return ts.strftime('%Y-%m-%d %H:%M:%S.%f')

def parse_unix_epoch(date_value):
    ts = datetime.datetime.fromtimestamp(date_value)
    return ts.strftime('%Y-%m-%d %H:%M:%S.%f')
```

我们的最后一个函数处理`SOFTWARE`配置单元，在控制台窗口中向用户呈现信息。此功能还可以从收集蜂巢的根开始，然后选择`Microsoft\Windows NT\CurrentVersion`键。此键包含有关 OS 安装元数据和其他有用子键的值。在此函数中，我们将提取`ProductName`、`CSDVersion`、`CurrentBuild number`、`RegisteredOwner`、`RegisteredOrganization`和`InstallDate`值。虽然这些值中的大多数是字符串，我们可以直接打印到控制台，但在打印之前，我们需要使用`Unix`历元转换器来解释安装日期值。

```py
def process_software_hive(hive):
    root = hive.root()
    nt_curr_ver = root.find_key("Microsoft").find_key(
        "Windows NT").find_key("CurrentVersion")

    print("Product name: {}".format(nt_curr_ver.value(
        "ProductName").value()))
    print("CSD Version: {}".format(nt_curr_ver.value(
        "CSDVersion").value()))
    print("Current Build: {}".format(nt_curr_ver.value(
        "CurrentBuild").value()))
    print("Registered Owner: {}".format(nt_curr_ver.value(
        "RegisteredOwner").value()))
    print("Registered Org: {}".format(nt_curr_ver.value(
        "RegisteredOrganization").value()))

    raw_install_date = nt_curr_ver.value("InstallDate").value()
    install_date = parse_unix_epoch(raw_install_date)
    print("Installation Date: {}".format(install_date))
```

当我们运行此脚本时，我们可以了解存储在我们解释的密钥中的信息：

![](../images/00101.jpeg)

# 还有更多。。。

这个脚本可以进一步改进。我们提供了以下一项或多项建议：

*   添加逻辑以处理在初始搜索中未找到`SYSTEM`或`SOFTWARE`蜂箱的情况
*   考虑添加对支持 To.t0 文件的支持，获取关于挂载点和 shell 包查询的基本信息
*   列出`System`配置单元中的基本 USB 设备信息
*   解析`SAM`配置单元以显示用户和组信息

# 收集用户活动

难度：中等

Python 版本：2.7

操作系统：Linux

Windows 存储了大量有关用户活动的信息，与其他注册表配置单元一样，`NTUSER.DAT`文件是调查过程中需要依赖的重要资源。此配置单元位于每个用户的配置文件中，并存储与系统上特定用户配置相关的信息和配置。

在这个配方中，我们介绍了`NTUSER.DAT`中的多个键，这些键可以揭示用户在系统上的行为。这包括以前在 Windows 资源管理器中运行的搜索、在资源管理器导航栏中键入的路径，以及最近在 Windows`run`命令中使用的语句。这些工件更好地说明了用户是如何与系统交互的，并且可以让用户了解系统的正常或异常使用情况。

# 开始

此配方需要安装四个第三方模块才能工作：`jinja2`、`pytsk3`、`pyewf`和`Registry`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第 8 章](08.html#75QNI0-260f9401d2714cb9ab693c4692308abe)、*使用法医证据容器**配方、*。同样，有关安装`Registry`的详细信息，请参阅*阅读注册表*配方中的*入门*部分。此脚本中使用的所有其他库都存在于 Python 的标准库中。

我们将重新引入[第 2 章](02.html#23MNU0-260f9401d2714cb9ab693c4692308abe)中首先介绍的`jinja2`*创建工件报告*R*ecipes*来构建 HTML 报告。该库是一种模板语言，允许我们使用 Pythonic 语法以编程方式构建文本文件。作为提醒，我们可以使用`pip`安装此库：

```py
pip install jinja2==2.9.6
```

# 怎么做。。。

要从图像中的`NTUSER.DAT`文件中提取这些值，我们必须：

1.  搜索整个系统中的所有`NTUSER.DAT`文件。
2.  解析每个`NTUSER.DAT`文件的`WordWheelQuery`键。
3.  读取每个`NTUSER.DAT`文件的`TypedPath`键。
4.  为每个`NTUSER.DAT`文件提取`RunMRU`密钥。
5.  将每个已处理的工件写入 HTML 报告。

# 它是如何工作的。。。

我们的进口开始方式与之前的配方相同，在`jinja2`模块中添加：

```py
from __future__ import print_function
from argparse import ArgumentParser
import os
import StringIO
import struct

from utility.pytskutil import TSKUtil
from Registry import Registry
import jinja2
```

此配方的命令行处理程序采用三个位置参数，`EVIDENCE_FILE`、`IMAGE_TYPE`和`REPORT`，分别表示证据文件的路径、证据文件的类型以及 HTML 报告的所需输出路径。这三个参数被传递给`main()`函数。

```py
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=__description__,
        epilog="Developed by {} on {}".format(
            ", ".join(__authors__), __date__)
    )
    parser.add_argument('EVIDENCE_FILE',
                        help="Path to evidence file")
    parser.add_argument('IMAGE_TYPE',
                        help="Evidence file format",
                        choices=('ewf', 'raw'))
    parser.add_argument('REPORT',
                        help="Path to report file")
    args = parser.parse_args()
    main(args.EVIDENCE_FILE, args.IMAGE_TYPE, args.REPORT)
```

`main()`功能首先读取证据文件并搜索所有`NTUSER.DAT`文件。接下来，我们设置了一个 dictionary 对象`nt_rec`，该对象虽然复杂，但其设计方式简化了 HTML 报告生成过程。然后，我们开始遍历发现的配置单元，并从路径中解析出用户名，以便在处理函数中引用。

```py
def main(evidence, image_type, report):
    tsk_util = TSKUtil(evidence, image_type)
    tsk_ntuser_hives = tsk_util.recurse_files('ntuser.dat',
                                              '/Users', 'equals')

    nt_rec = {
        'wordwheel': {'data': [], 'title': 'WordWheel Query'},
        'typed_path': {'data': [], 'title': 'Typed Paths'},
        'run_mru': {'data': [], 'title': 'Run MRU'}
    }
    for ntuser in tsk_ntuser_hives:
        uname = ntuser[1].split("/")[1]
```

接下来，我们传递要作为`Registry`对象打开的`pytsk`文件句柄。该结果对象用于收集与所有所需值（`Software\Microsoft\Windows\CurrentVersion\Explorer`相同的`root`键。如果找不到此密钥路径，我们将继续下一个`NTUSER.DAT`文件。

```py
        open_ntuser = open_file_as_reg(ntuser[2])
        try:
            explorer_key = open_ntuser.root().find_key(
                "Software").find_key("Microsoft").find_key(
                    "Windows").find_key("CurrentVersion").find_key(
                        "Explorer")
        except Registry.RegistryKeyNotFoundException:
            continue # Required registry key not found for user
```

如果找到它们的密钥，我们将调用负责每个工件的三个处理函数，并提供共享密钥对象和用户名。返回的数据存储在字典中相应的数据键中。我们可以通过扩展存储对象定义并添加一个与此处所示的其他函数具有相同配置文件的新函数，轻松扩展代码解析的工件数量：

```py
        nt_rec['wordwheel']['data'] += parse_wordwheel(
            explorer_key, uname)
        nt_rec['typed_path']['data'] += parse_typed_paths(
            explorer_key, uname)
        nt_rec['run_mru']['data'] += parse_run_mru(
            explorer_key, uname)
```

在遍历`NTUSER.DAT`文件之后，我们通过提取数据列表中第一项的键列表来为每种记录类型设置标题。由于数据列表中的所有字典对象都有统一的键，因此我们可以使用此方法来减少传递的参数或变量的数量。这些语句也很容易扩展。

```py
    nt_rec['wordwheel']['headers'] = \
        nt_rec['wordwheel']['data'][0].keys()

    nt_rec['typed_path']['headers'] = \
        nt_rec['typed_path']['data'][0].keys()

    nt_rec['run_mru']['headers'] = \
        nt_rec['run_mru']['data'][0].keys()
```

最后，我们将完成的 dictionary 对象与报告文件的路径一起传递给我们的`write_html()`方法：

```py
    write_html(report, nt_rec)
```

我们以前在前面的配方中见过`open_file_as_reg()`方法。作为提醒，它接受`pytsk`文件句柄并通过`StringIO`类将其读入`Registry`类。返回的`Registry`对象允许我们以面向对象的方式交互和读取注册表。

```py
def open_file_as_reg(reg_file):
    file_size = reg_file.info.meta.size
    file_content = reg_file.read_random(0, file_size)
    file_like_obj = StringIO.StringIO(file_content)
    return Registry.Registry(file_like_obj)
```

第一个处理函数处理`WordWheelQuery`键，该键存储有关用户在 Windows 资源管理器中运行的搜索的信息。我们可以通过从`explorer_key`对象按名称访问密钥来解析这个工件。如果键不存在，我们将返回一个空列表，因为我们没有任何要提取的值。

```py
def parse_wordwheel(explorer_key, username):
    try:
        wwq = explorer_key.find_key("WordWheelQuery")
    except Registry.RegistryKeyNotFoundException:
        return []
```

另一方面，如果键存在，我们将遍历`MRUListEx`值，该值包含包含搜索顺序的整数列表。列表中的每个数字都与键中相同数字的值匹配。因此，我们读取列表的顺序，并按其出现的顺序解释其余值。每个值名都存储为一个两字节的整数，因此我们将此列表拆分为两个字节的块，并使用`struct`读取整数。然后，在检查该值是否不存在后，将其附加到列表中。如果列表中确实存在，并且是`\x00`或`\xFF`，则我们已经到达`MRUListEx`数据的末尾，并跳出循环：

```py
    mru_list = wwq.value("MRUListEx").value()
    mru_order = []
    for i in xrange(0, len(mru_list), 2):
        order_val = struct.unpack('h', mru_list[i:i + 2])[0]
        if order_val in mru_order and order_val in (0, -1):
            break
        else:
            mru_order.append(order_val)
```

使用我们的有序值列表，我们对其进行迭代，以按照搜索词的运行顺序提取搜索词。因为我们知道使用顺序，所以可以将`WordWheelQuery`键的最后写入时间关联为搜索项的时间戳。此时间戳仅与最近运行的搜索关联。所有其他搜索的值均为`N/A`。

```py
    search_list = []
    for count, val in enumerate(mru_order):
        ts = "N/A"
        if count == 0:
            ts = wwq.timestamp()
```

然后，我们在`append`语句中构建字典，添加时间值、用户名、顺序（作为计数整数）、值的名称和搜索内容。为了正确显示搜索内容，我们需要以字符串形式提供密钥名称，并将文本解码为 UTF-16。此文本一旦去掉空终止符，就可以用于报告了。在处理完所有值并最终返回之前，将生成该列表。

```py
        search_list.append({
            'timestamp': ts,
            'username': username,
            'order': count,
            'value_name': str(val),
            'search': wwq.value(str(val)).value().decode(
                "UTF-16").strip("\x00")
        })
    return search_list
```

下一个处理函数处理类型化路径键，使用与前一个处理函数相同的参数。我们以同样的方式访问密钥，如果没有找到`TypedPaths`子密钥，则返回空列表。

```py
def parse_typed_paths(explorer_key, username):
    try:
        typed_paths = explorer_key.find_key("TypedPaths")
    except Registry.RegistryKeyNotFoundException:
        return []
```

该键没有对类型化路径排序的 MRU 值，因此我们读取其所有值并将其直接添加到列表中。我们可以从此键收集值的名称和路径，并为其他上下文添加用户名值。我们通过将字典值列表返回给`main()`函数来完成此函数。

```py
    typed_path_details = []
    for val in typed_paths.values():
        typed_path_details.append({
            "username": username,
            "value_name": val.name(),
            "path": val.value()
        })
    return typed_path_details
```

最后一个处理函数处理`RunMRU`键。如果它在`explorer_key`中不存在，我们返回一个空列表，如前所示。

```py
def parse_run_mru(explorer_key, username):
    try:
        run_mru = explorer_key.find_key("RunMRU")
    except Registry.RegistryKeyNotFoundException:
        return []
```

由于这个键可以是空的，我们首先检查是否有要解析的值，如果没有，则返回一个空列表以防止任何不必要的处理。

```py
    if len(run_mru.values()) == 0:
        return []
```

与`WordWheelQuery`类似，该键也有一个 MRU 值，我们通过处理该值来了解其他值的正确顺序。此列表以不同的方式存储项，因为其值是字母而不是整数。这使得我们的工作非常简单，因为我们直接使用这些字符查询必要的值，而无需额外处理。我们将值的顺序附加到列表中，然后继续。

```py
    mru_list = run_mru.value("MRUList").value()
    mru_order = []
    for i in mru_list:
        mru_order.append(i)
```

当我们遍历值的顺序时，我们开始构建结果字典。首先，我们以与`WordWheelQuery`处理器相同的方式处理时间戳，方法是分配一个默认`N/A`值，并使用键的上次写入时间（如果它是我们有序列表中的第一个条目）更新它。接下来，我们附加一个包含相关条目的字典，如用户名、值顺序、值名称和值内容。处理完`Run`键中的所有剩余值后，将返回此字典列表。

```py
    mru_details = []
    for count, val in enumerate(mru_order):
        ts = "N/A"
        if count == 0:
            ts = run_mru.timestamp()
        mru_details.append({
            "username": username,
            "timestamp": ts,
            "order": count,
            "value_name": val,
            "run_statement": run_mru.value(val).value()
        })

    return mru_details
```

最后一个函数处理 HTML 报告的创建。此函数首先准备代码路径和`jinja2`环境类。这个类用于存储库中的共享资源，我们使用它将库指向它应该搜索模板文件的目录。在本例中，我们希望它在当前目录中查找模板 HTML 文件，因此我们使用`os`库获取当前工作目录，并将其提供给`FileSystemLoader()`类。

```py
def write_html(outfile, data_dict):
    cwd = os.path.dirname(os.path.abspath(__file__))
    env = jinja2.Environment(loader=jinja2.FileSystemLoader(cwd))
```

配置好环境后，我们调用想要使用的模板，然后调用`render()`方法，用传递的字典创建 HTML 文件。`render`函数返回一个字符串，该字符串表示呈现的 HTML 输出，其中包含我们写入输出文件的已处理数据的结果。

```py
    template = env.get_template("user_activity.html")
    rendering = template.render(nt_data=data_dict)
    with open(outfile, 'w') as open_outfile:
        open_outfile.write(rendering)
```

让我们看看模板文件，它以任何带有`html`、`head`和`body`标记的 HTML 文档开始。虽然我们在`head`标签中包含了脚本和样式表，但我们在这里省略了不相关的内容。此信息可在代码包中完整查看。

我们用一个保存处理过的数据表和节头的`div`开始 HTML 文档。为了简化需要编写的 HTML 量，我们使用一个`for`循环从`nt_data`值收集每个嵌套字典。`jinja2`模板语言允许我们仍然使用 Python 循环，只要它们用花括号、百分比符号和空格字符包装。我们还可以引用对象的属性和方法，这样我们就可以在不需要额外代码的情况下遍历`nt_data`字典的值。

另一种常用的模板语法显示在`h2`标记中，在这里我们访问在`main()`函数中设置的 title 属性。我们希望`jinja2`引擎解释的变量（而不是显示为文字字符串）需要用双花括号和空格字符括起来。现在将打印`nt_data`词典中每个章节的章节标题。

```py
<html> 
<head>...</head> 
<body> 
    <div class="container"> 
        {% for nt_content in nt_data.values() %} 
            <h2>{{ nt_content['title'] }}</h2> 
```

在这个循环中，我们使用`data`标记设置数据表，并创建一个新行来保存表头。为了生成标题，我们逐步遍历我们收集的每个标题，并在嵌套的`for`循环中分配值。注意我们需要如何用`endfor`语句指定循环的结尾；这是模板引擎所必需的，因为（与 Python 不同）它对缩进不敏感：

```py
            <table class="table table-hover table-condensed"> 
                <tr> 
                    {% for header in nt_content['headers'] %} 
                        <th>{{ header }}</th> 
                    {% endfor %} 
                <tr/> 
```

在表头之后，我们进入一个单独的循环来迭代数据列表中的每个字典。在每个表行中，我们使用与表头类似的逻辑创建另一个`for`循环，将每个值写入行中的一个单元格：

```py
                {% for entry in nt_content['data'] %} 
                    <tr> 
                        {% for header in nt_content['headers'] %} 
                            <td>{{ entry[header] }}</td> 
                        {% endfor %} 
                    </tr> 
```

现在填充了 HTML 数据表，我们关闭了当前数据点的`for`循环：我们画一条水平线，开始编写下一个工件的数据表。一旦我们完全遍历了这些内容，我们就关闭了外部`for`循环和在 HTML 报告开始时打开的标记。

```py
                {% endfor %} 
            </table> 
            <br /> 
            <hr /> 
            <br /> 
        {% endfor %} 
    </div> 
</body> 
</html> 
```

我们生成的报告如下：

![](../images/00102.jpeg)

# 还有更多。。。

这个脚本可以进一步改进。我们提供了以下一项或多项建议：

*   在仪表板中添加额外的`NTUser`或其他易于查看的工件，以提供更有用的信息
*   使用各种 JavaScript 和 CSS 元素将图表、时间线或其他交互元素添加到此仪表板
*   提供从仪表板到 CSV 或 Excel 电子表格的导出选项以及附加 JavaScript

# 缺失的环节

难度：中等

Python 版本：2.7

操作系统：Linux

快捷方式文件，也称为链接文件，在操作系统平台上很常见。它们允许用户使用一个文件引用位于系统其他位置的另一个文件。在 Windows 平台上，这些链接文件还记录对它们引用的文件的历史访问。通常，链接文件的创建时间表示具有该名称的文件的第一次访问时间，修改时间表示具有该名称的文件的最近访问时间。利用这个，我们可以推断出一个活动窗口，并了解如何以及在何处访问这些文件。

# 开始

此配方需要安装三个第三方模块才能工作：`pytsk3`、`pyewf`和`pylnk`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第 8 章](08.html#75QNI0-260f9401d2714cb9ab693c4692308abe)、*使用法医证据容器**配方*。此脚本中使用的所有其他库都存在于 Python 的标准库中。

导航到 GitHub 存储库并下载所需版本的`pylnk`库。此配方是使用`pylnk-alpha-20170111`版本开发的。接下来，提取版本内容后，打开终端并导航到提取的目录并执行以下命令：

```py
./synclibs.sh
./autogen.sh
sudo python setup.py install
```

To learn more about the `pylnk` library, visit [https://github.com/libyal/liblnk](https://github.com/libyal/liblnk).

最后，我们可以通过打开 Python 解释器、导入`pylnk`，并运行`gpylnk.get_version()`方法来检查库的安装情况，以确保获得正确的发行版本。

# 怎么做。。。

此脚本将利用以下步骤：

1.  搜索系统中的所有`lnk`文件。
2.  遍历发现的`lnk`文件并提取相关属性。
3.  将所有工件写入 CSV 报告。

# 它是如何工作的。。。

从导入开始，我们引入了侦探工具包实用程序和`pylnk`库。我们还引入了用于参数解析、编写 CSV 报告和`StringIO`以文件形式读取 Sleuth Kit 对象的库：

```py
from __future__ import print_function
from argparse import ArgumentParser
import csv
import StringIO

from utility.pytskutil import TSKUtil
import pylnk
```

此配方的命令行处理程序采用三个位置参数，`EVIDENCE_FILE`、`IMAGE_TYPE`和`CSV_REPORT`，分别表示证据文件的路径、证据文件的类型以及 CSV 报告的所需输出路径。这三个参数被传递给`main()`函数。

```py
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=__description__,
        epilog="Developed by {} on {}".format(
            ", ".join(__authors__), __date__)
    )
    parser.add_argument('EVIDENCE_FILE', help="Path to evidence file")
    parser.add_argument('IMAGE_TYPE', help="Evidence file format",
                        choices=('ewf', 'raw'))
    parser.add_argument('CSV_REPORT', help="Path to CSV report")
    args = parser.parse_args()
    main(args.EVIDENCE_FILE, args.IMAGE_TYPE, args.CSV_REPORT)
```

`main()`函数首先创建用于解释证据文件的`TSKUtil`对象，并遍历文件系统以查找以`lnk`结尾的文件。如果在系统上找不到任何`lnk`文件，脚本会提醒用户并退出。否则，我们将为每个`lnk`文件指定表示要存储的数据属性的列。虽然还有其他可用属性，但我们在此配方中提取了一些更相关的属性：

```py
def main(evidence, image_type, report):
    tsk_util = TSKUtil(evidence, image_type)
    lnk_files = tsk_util.recurse_files("lnk", path="/", logic="endswith")
    if lnk_files is None:
        print("No lnk files found")
        exit(0)

    columns = [
        'command_line_arguments', 'description', 'drive_serial_number',
        'drive_type', 'file_access_time', 'file_attribute_flags',
        'file_creation_time', 'file_modification_time', 'file_size',
        'environmental_variables_location', 'volume_label',
        'machine_identifier', 'local_path', 'network_path',
        'relative_path', 'working_directory'
    ]
```

接下来，我们遍历发现的`lnk`文件，使用`open_file_as_lnk()`函数将每个文件作为一个文件打开。返回的对象是`pylnk`库的一个实例，我们可以从中读取属性。我们使用文件名和路径初始化属性字典，然后遍历我们在`main()`函数中指定的列。对于每一列，我们尝试读取指定的属性值，如果无法读取，则存储一个“`N/A`”值。这些属性存储在`lnk_data`字典中，一旦提取了所有属性，字典就会附加到`parsed_lnks`列表中。完成每个`lnk`文件的此过程后，我们将此列表连同输出路径和列名一起传递给`write_csv()`方法。

```py
    parsed_lnks = []
    for entry in lnk_files:
        lnk = open_file_as_lnk(entry[2])
        lnk_data = {'lnk_path': entry[1], 'lnk_name': entry[0]}
        for col in columns:
            lnk_data[col] = getattr(lnk, col, "N/A")
        lnk.close()
        parsed_lnks.append(lnk_data)

    write_csv(report, columns + ['lnk_path', 'lnk_name'], parsed_lnks)
```

为了将我们的`pytsk`文件对象作为`pylink`对象打开，我们使用`open_file_as_lnk()`函数，该函数与本章中其他类似命名的函数一样工作。此函数使用`read_random()`方法和文件大小属性将整个文件读取到`StringIO`缓冲区中，然后将该缓冲区传递到`pylnk`文件对象中。以这种方式读取允许我们将数据作为文件读取，而无需将其缓存到磁盘。将文件加载到`lnk`对象后，我们将其返回到`main()`函数：

```py
def open_file_as_lnk(lnk_file):
    file_size = lnk_file.info.meta.size
    file_content = lnk_file.read_random(0, file_size)
    file_like_obj = StringIO.StringIO(file_content)
    lnk = pylnk.file()
    lnk.open_file_object(file_like_obj)
    return lnk
```

最后一个函数是 common CSV writer，它使用`csv.DictWriter`类迭代数据结构，并将相关字段写入电子表格。`main()`函数中定义的列列表的顺序决定了它们在这里作为`fieldnames`参数的顺序。如果需要，可以更改该顺序，以修改它们在生成的电子表格中的显示顺序。

```py
def write_csv(outfile, fieldnames, data):
    with open(outfile, 'wb') as open_outfile:
        csvfile = csv.DictWriter(open_outfile, fieldnames)
        csvfile.writeheader()
        csvfile.writerows(data)
```

运行脚本后，我们可以在单个 CSV 报告中查看结果，如以下两个屏幕截图所示。由于有许多可见列，出于可读性考虑，我们选择只显示少数列：

![](../images/00103.jpeg)

![](../images/00104.jpeg)

# 还有更多。。。

这个脚本可以进一步改进。我们提供了以下一项或多项建议：

*   添加检查以查看目标文件是否仍然存在
*   确定远程卷或可移动卷上的目标位置
*   添加对分析跳转列表的支持

# 到处寻找

菜谱难度：难

Python 版本：2.7

操作系统：Linux

大多数现代操作系统都维护存储在系统上的文件和其他数据内容的索引。这些索引允许对系统卷上的文件格式、电子邮件和其他内容进行更高效的搜索。在 Windows 上，可以在`Windows.edb`文件中找到这样的索引。该数据库以**可扩展存储引擎**（**ESE**文件格式存储，并在`ProgramData`目录中找到。我们将利用`libyal`项目中的另一个库来解析此文件，以提取有关系统上索引内容的信息。

# 开始

此配方需要安装四个第三方模块才能工作：`pytsk3`、`pyewf`、`pyesedb`和`unicodecsv`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第 8 章](08.html#75QNI0-260f9401d2714cb9ab693c4692308abe)、*使用法医证据容器**配方*。同样，有关安装`unicodecsv`的详细信息，请参阅*一人垃圾是法医宝藏*配方中的*入门*部分。此脚本中使用的所有其他库都存在于 Python 的标准库中。

导航到 GitHub 存储库并下载每个库所需的版本。此配方是使用`libesedb-experimental-20170121`版本开发的。提取版本内容后，打开终端，导航到提取的目录，并执行以下命令：

```py
./synclibs.sh
./autogen.sh
sudo python setup.py install 
```

To learn more about the `pyesedb` library, visit [**https://github.com/libyal/libesedb**](https://github.com/libyal/libesedb)**.**

最后，我们可以通过打开 Python 解释器、导入`pyesedb`，并运行`epyesedb.get_version()`方法来检查库的安装情况，以确保获得正确的发行版本。

# 怎么做。。。

要起草此脚本，我们需要：

1.  递归`ProgramData`目录以搜索`Windows.edb`文件。
2.  遍历发现的`Windows.edb`文件（虽然实际上应该只有一个），并使用`pyesedb`库打开文件。
3.  处理每个文件以提取关键列和属性。
4.  将这些关键列和属性写入报告。

# 它是如何工作的。。。

这里的导入包括我们在本章中用于参数解析、字符串缓冲区文件类对象和`TSK`实用程序的大多数方法的库。我们还导入了`unicodecsv`库来处理 CSV 报告中的任何 Unicode 对象，`datetime`库来协助时间戳解析，`struct`模块来帮助理解我们读取的二进制数据。此外，我们还定义了一个全局变量`COL_TYPES`，它为`pyesedb`库中的列类型添加别名，用于帮助识别我们稍后将在代码中提取的数据类型：

```py
from __future__ import print_function
from argparse import ArgumentParser
import unicodecsv as csv
import datetime
import StringIO
import struct

from utility.pytskutil import TSKUtil
import pyesedb

COL_TYPES = pyesedb.column_types
```

此配方的命令行处理程序采用三个位置参数，`EVIDENCE_FILE`、`IMAGE_TYPE`和`CSV_REPORT`，分别表示证据文件的路径、证据文件的类型以及 CSV 报告的所需输出路径。这三个参数被传递给`main()`函数。

```py
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=__description__,
        epilog="Developed by {} on {}".format(
            ", ".join(__authors__), __date__)
    )
    parser.add_argument('EVIDENCE_FILE', help="Path to evidence file")
    parser.add_argument('IMAGE_TYPE', help="Evidence file format",
                        choices=('ewf', 'raw'))
    parser.add_argument('CSV_REPORT', help="Path to CSV report")
    args = parser.parse_args()
    main(args.EVIDENCE_FILE, args.IMAGE_TYPE, args.CSV_REPORT)
```

`main()`功能打开证据并在`ProgramData`目录中搜索`Windows.edb`文件。如果找到一个或多个文件，我们将遍历列表并打开每个 ESE 数据库，以便使用`process_windows_search()`函数进行进一步处理。此函数用于返回要使用的电子表格列标题以及包含要包含在报告中的数据的词典列表。然后通过`write_csv()`方法将该信息写入输出 CSV 进行审查：

```py
def main(evidence, image_type, report):
    tsk_util = TSKUtil(evidence, image_type)
    esedb_files = tsk_util.recurse_files(
        "Windows.edb",
        path="/ProgramData/Microsoft/Search/Data/Applications/Windows",
        logic="equals"
    )
    if esedb_files is None:
        print("No Windows.edb file found")
        exit(0)

    for entry in esedb_files:
        ese = open_file_as_esedb(entry[2])
        if ese is None:
            continue # Invalid ESEDB
        report_cols, ese_data = process_windows_search(ese)

    write_csv(report, report_cols, ese_data)
```

读取响应 ESE 数据库需要`open_file_as_esedb()`功能。此代码块使用与前面的方法类似的逻辑将文件读入`StringIO`对象，并使用库打开类似文件的对象。请注意，如果文件太大或计算机内存较低，这可能会导致系统出错。您可以使用内置的`tempfile`库将文件缓存到磁盘上的临时位置，如果您愿意，可以从那里读取。

```py
def open_file_as_esedb(esedb):
    file_size = esedb.info.meta.size
    file_content = esedb.read_random(0, file_size)
    file_like_obj = StringIO.StringIO(file_content)
    esedb = pyesedb.file()
    try:
        esedb.open_file_object(file_like_obj)
    except IOError:
        return None
    return esedb
```

我们的`process_windows_search()`函数以列定义开始。虽然我们之前的方法使用了一个简单的列列表，`pyesedb`库将列索引作为输入，从表中的行中检索值。因此，我们的列列表必须由元组组成，其中第一个元素是数字（索引），第二个是字符串描述。由于函数中没有使用描述来选择列，因此我们以希望在报告中显示的方式命名这些列。对于此配方，我们定义了以下列索引和名称：

```py
def process_windows_search(ese):
    report_cols = [
        (0, "DocID"), (286, "System_KindText"),
        (35, "System_ItemUrl"), (5, "System_DateModified"),
        (6, "System_DateCreated"), (7, "System_DateAccessed"),
        (3, "System_Size"), (19, "System_IsFolder"),
        (2, "System_Search_GatherTime"), (22, "System_IsDeleted"),
        (61, "System_FileOwner"), (31, "System_ItemPathDisplay"),
        (150, "System_Link_TargetParsingPath"),
        (265, "System_FileExtension"), (348, "System_ComputerName"),
        (34, "System_Communication_AccountName"),
        (44, "System_Message_FromName"),
        (43, "System_Message_FromAddress"), (49, "System_Message_ToName"),
        (47, "System_Message_ToAddress"),
        (62, "System_Message_SenderName"),
        (189, "System_Message_SenderAddress"),
        (52, "System_Message_DateSent"),
        (54, "System_Message_DateReceived")
    ]
```

定义感兴趣的列之后，我们访问`SystemIndex_0A`表，其中包含索引文件、邮件和其他条目。我们遍历表中的记录，为每个记录构建一个列值的`record_info`字典，最终将附加到`table_data`列表中。第二个循环遍历我们前面定义的列，并尝试提取记录中每个列的值和值类型。

```py
    table = ese.get_table_by_name("SystemIndex_0A")
    table_data = []
    for record in table.records:
        record_info = {}
        for col_id, col_name in report_cols:
            rec_val = record.get_value_data(col_id)
            col_type = record.get_column_type(col_id)
```

使用前面定义的`COL_TYPES`全局变量，我们可以引用各种数据类型，并确保正确解释值。下面代码块中的逻辑侧重于根据值的数据类型正确解释值。首先，我们处理日期，它可以存储为 Windows`FILETIME`值。如果可能，我们尝试转换`FILETIME`值，如果不可能，则以十六进制表示日期值。下一条语句检查文本值，用`pyesedb``get_value_data_as_string()`函数解释该值，或将其解释为 UTF-16 大端字符，并替换任何无法识别的字符以确保完整性。

然后，我们分别使用`pyesedb``get_value_data_as_integer()`函数和简单的比较语句分别处理整数和布尔数据类型解释。具体地说，我们检查`rec_val`是否等于`"\x01"`，并允许基于该比较设置`rec_val``True`或`False`。如果这些数据类型都无效，我们将值解释为十六进制，并在将值追加到表之前将其与相关列名一起存储：

```py
            if col_type in (COL_TYPES.DATE_TIME, COL_TYPES.BINARY_DATA):
                try:
                    raw_val = struct.unpack('>q', rec_val)[0]
                    rec_val = parse_windows_filetime(raw_val)
                except Exception:
                    if rec_val is not None:
                        rec_val = rec_val.encode('hex')

            elif col_type in (COL_TYPES.TEXT, COL_TYPES.LARGE_TEXT):
                try:
                    rec_val = record.get_value_data_as_string(col_id)
                except Exception:
                    rec_val = rec_val.decode("utf-16-be", "replace")

            elif col_type == COL_TYPES.INTEGER_32BIT_SIGNED:
                rec_val = record.get_value_data_as_integer(col_id)

            elif col_type == COL_TYPES.BOOLEAN:
                rec_val = rec_val == '\x01'

            else:
                if rec_val is not None:
                    rec_val = rec_val.encode('hex')

            record_info[col_name] = rec_val
        table_data.append(record_info)
```

然后，我们向调用函数返回一个元组，其中第一个元素是`report_cols`字典中列的名称列表，第二个元素是数据字典列表。

```py
    return [x[1] for x in report_cols], table_data
```

借用[第 7 章](07.html#6A5N80-260f9401d2714cb9ab693c4692308abe)、*基于日志的工件配方*中的日期解析配方的逻辑，我们实现了一个将窗口`FILETIME`值解析为人类可读状态的函数。它接受一个整数值作为输入，并返回一个人类可读的字符串：

```py
def parse_windows_filetime(date_value):
    microseconds = float(date_value) / 10
    ts = datetime.datetime(1601, 1, 1) + datetime.timedelta(
        microseconds=microseconds)
    return ts.strftime('%Y-%m-%d %H:%M:%S.%f')
```

最后一个函数是 CSV 报告编写器，它使用`DictWriter`类将收集信息的列和行写入打开的 CSV 电子表格。虽然我们从一开始就选择了可用列的子集，但在不同的案例类型中，还有更多的可用列可供选择。因此，我们建议查看所有可用列，以便更好地了解此配方以及哪些列可能对您有用，哪些列可能对您不有用。

```py
def write_csv(outfile, fieldnames, data):
    with open(outfile, 'wb') as open_outfile:
        csvfile = csv.DictWriter(open_outfile, fieldnames)
        csvfile.writeheader()
        csvfile.writerows(data)
```

运行配方后，我们可以查看此处显示的输出 CSV。由于本报告有许多专栏，我们在以下两个屏幕截图中突出显示了几个有趣的专栏：

![](../images/00105.jpeg)

![](../images/00106.jpeg)

# 还有更多。。。

这个脚本可以进一步改进。我们提供了以下一项或多项建议：

*   添加支持以检查引用的文件和文件夹是否存在
*   在使用 Python`tempfile`库解析大型数据库时，将`Windows.edb`文件写入临时位置以减轻内存压力
*   使用表中超过 300 个可用列中的更多列添加更多列或创建单独（目标）报告