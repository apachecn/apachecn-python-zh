# 划分文本数据并构建文本分类器

本章介绍以下主题：

*   构建文本分类器
*   使用标记化预处理数据
*   词干文本数据
*   使用组块划分文本
*   构建一个单词包模型
*   文本分类器的应用

# 介绍

本章介绍构建文本分类器的方法。这包括从数据库中提取重要特征、训练、测试和验证文本分类器。首先，使用常用词训练文本分类器。然后，使用训练好的文本分类器进行预测。构建文本分类器包括使用标记化对数据进行预处理、对文本数据进行词干分析、使用组块划分文本以及构建单词包模型。

# 构建文本分类器

分类器单元通常被认为是将数据库划分为不同的类。朴素贝叶斯分类器方案在文献中被广泛考虑用于基于训练模型的文本分离。本章的这一部分首先考虑一个包含关键字的文本数据库；特征提取从文本中提取关键短语并训练分类器系统。然后，执行**项频率逆文档频率**（**tf idf**转换来指定单词的重要性。最后，使用分类器系统对输出进行预测和打印。

# 怎么做。。。

1.  在新的 Python 文件中包含以下行以添加数据集：

```py
from sklearn.datasets import fetch_20newsgroups 
category_mapping = {'misc.forsale': 'Sellings', 'rec.motorcycles': 'Motorbikes', 
        'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography', 
        'sci.space': 'OuterSpace'} 

training_content = fetch_20newsgroups(subset='train', 
categories=category_mapping.keys(), shuffle=True, random_state=7) 
```

2.  执行特征提取以从文本中提取主要单词：

```py
from sklearn.feature_extraction.text import CountVectorizer 

vectorizing = CountVectorizer() 
train_counts = vectorizing.fit_transform(training_content.data) 
print "nDimensions of training data:", train_counts.shape 
```

3.  培训分类器：

```py
from sklearn.naive_bayes import MultinomialNB 
from sklearn.feature_extraction.text import TfidfTransformer 

input_content = [ 
    "The curveballs of right handed pitchers tend to curve to the left", 
    "Caesar cipher is an ancient form of encryption", 
    "This two-wheeler is really good on slippery roads" 
] 

tfidf_transformer = TfidfTransformer() 
train_tfidf = tfidf_transformer.fit_transform(train_counts) 
```

4.  实现多项式朴素贝叶斯分类器：

```py
classifier = MultinomialNB().fit(train_tfidf, training_content.target) 
input_counts = vectorizing.transform(input_content) 
input_tfidf = tfidf_transformer.transform(input_counts) 
```

5.  预测产出类别：

```py
categories_prediction = classifier.predict(input_tfidf) 
```

6.  打印输出：

```py
for sentence, category in zip(input_content, categories_prediction): 
    print 'nInput:', sentence, 'nPredicted category:',  
            category_mapping[training_content.target_names[category]] 
```

以下屏幕截图提供了基于数据库输入预测对象的示例：

![](Images/14d70abf-d300-4e88-9ece-6318c2e1abb0.png)

# 它是如何工作的。。。

本章的前一节提供了有关实现的分类器部分和一些示例结果的见解。分类器部分基于训练的朴素贝叶斯中先前文本与测试序列*中的关键测试之间的比较来工作。*

# 另见

请参阅以下文章：

*   *情绪分析算法及应用：在[的](https://www.sciencedirect.com/science/article/pii/S2090447914000550)*调查 https://www.sciencedirect.com/science/article/pii/S2090447914000550 。

*   S*在线评论的实体分类：使用基于句子的语言模型*学习情感预测在[的工作原理 https://www.tandfonline.com/doi/abs/10.1080/0952813X.2013.782352?src=recsys &日志代码=teta20](https://www.tandfonline.com/doi/abs/10.1080/0952813X.2013.782352?src=recsys&journalCode=teta20)。

*   *使用产品评论数据*和*在存在模式*的情况下进行句子级情感分析，以了解更多关于[推荐系统中使用的各种指标的信息 https://journalofbigdata.springeropen.com/articles/10.1186/s40537-015-0015-2](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-015-0015-2) 和；[https://link.springer.com/chapter/10.1007/978-3-642-54903-8_1](https://link.springer.com/chapter/10.1007/978-3-642-54903-8_1) 。

# 使用标记化预处理数据

数据预处理包括将现有文本转换为学习算法可接受的信息。

标记化是将文本划分为一组有意义的片段的过程。这些东西叫做代币。

# 怎么做。。。

1.  引入句子标记化：

```py
from nltk.tokenize import sent_tokenize
```

2.  形成新的文本标记器：

```py
tokenize_list_sent = sent_tokenize(text)
print "nSentence tokenizer:" 
print tokenize_list_sent 
```

3.  形成一个新词标记器：

```py
from nltk.tokenize import word_tokenize 
print "nWord tokenizer:" 
print word_tokenize(text) 
```

4.  引入一个新的 WordPunct 标记器：

```py
from nltk.tokenize import WordPunctTokenizer 
word_punct_tokenizer = WordPunctTokenizer() 
print "nWord punct tokenizer:" 
print word_punct_tokenizer.tokenize(text) 
```

标记器获得的结果如下所示。它把一个句子分成几个词组：

![](Images/fb4eb393-47a2-496c-a3cb-a2f99a36dfa6.png)

# 词干文本数据

词干分析过程包括为标记器的单词创建一个适当的单词，并减少字母。

# 怎么做。。。

1.  使用新的 Python 文件初始化词干分析过程：

```py
from nltk.stem.porter import PorterStemmer 
from nltk.stem.lancaster import LancasterStemmer 
from nltk.stem.snowball import SnowballStemmer 
```

2.  让我们来描述一些要考虑的词，如下：

```py
words = ['ability', 'baby', 'college', 'playing', 'is', 'dream', 'election', 'beaches', 'image', 'group', 'happy'] 
```

3.  确定要使用的一组`stemmers`：

```py
stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL'] 
```

4.  为所选的`stemmers`初始化必要的任务：

```py
stem_porter = PorterStemmer() 
stem_lancaster = LancasterStemmer() 
stem_snowball = SnowballStemmer('english') 
```

5.  设置表格格式以打印结果：

```py
formatted_row = '{:>16}' * (len(stemmers) + 1) 
print 'n', formatted_row.format('WORD', *stemmers), 'n' 
```

6.  反复检查单词列表，并使用所选的`stemmers`进行排列：

```py
for word in words:
  stem_words = [stem_porter.stem(word), 
  stem_lancaster.stem(word), 
  stem_snowball.stem(word)] 
  print formatted_row.format(word, *stem_words) 
```

从堵塞过程中获得的结果显示在以下屏幕截图中：

![](Images/c6b683e2-a8d2-4273-89f8-7b841628e28b.png)

# 使用组块划分文本

分块过程可用于将大文本划分为小的、有意义的单词。

# 怎么做。。。

1.  使用 Python 开发和导入以下包：

```py
import numpy as np 
from nltk.corpus import brown 
```

2.  描述一个将文本分成块的函数：

```py
# Split a text into chunks 
def splitter(content, num_of_words): 
   words = content.split(' ') 
   result = [] 
```

3.  初始化以下编程行以获取分配的变量：

```py
   current_count = 0 
   current_words = []
```

4.  使用以下文字开始迭代：

```py
   for word in words: 
     current_words.append(word) 
     current_count += 1 
```

5.  在获得基本字数后，重新组织变量：

```py
     if current_count == num_of_words: 
       result.append(' '.join(current_words)) 
       current_words = [] 
       current_count = 0 
```

6.  将块附加到输出变量：

```py
       result.append(' '.join(current_words)) 
       return result 
```

7.  导入 AutoT0 的数据，考虑第一个词：

```py
if __name__=='__main__': 
  # Read the data from the Brown corpus 
  content = ' '.join(brown.words()[:10000]) 
```

8.  描述每个块中的单词大小：

```py
  # Number of words in each chunk 
  num_of_words = 1600 
```

9.  启动一对重要变量：

```py
  chunks = [] 
  counter = 0 
```

10.  调用`splitter`函数打印结果：

```py
  num_text_chunks = splitter(content, num_of_words) 
  print "Number of text chunks =", len(num_text_chunks) 
```

11.  分块后获得的结果显示在以下屏幕截图中：

![](Images/e33566a4-1380-4420-91db-00b582161836.png)

# 构建一个单词包模型

当处理包含大词的文本文档时，我们需要将它们切换到几种类型的算术描述。我们需要将它们表述为适合机器学习算法。这些算法需要算术信息，以便能够检查数据并提供重要的细节。单词袋程序帮助我们实现这一点。单词包创建一个文本模型，该模型使用文档中的所有单词来发现词汇。之后，它通过构建文本中所有单词的直方图来为每个文本创建模型。

# 怎么做。。。

1.  通过导入以下文件初始化新的 Python 文件：

```py
import numpy as np 
from nltk.corpus import brown 
from chunking import splitter 
```

2.  定义`main`功能，从`Brown corpus`读取输入数据：

```py
if __name__=='__main__': 
        content = ' '.join(brown.words()[:10000]) 
```

3.  将文本内容拆分为块：

```py
    num_of_words = 2000 
    num_chunks = [] 
    count = 0 
    texts_chunk = splitter(content, num_of_words) 
```

4.  基于这些`text`块构建词汇表：

```py
    for text in texts_chunk: 
      num_chunk = {'index': count, 'text': text} 
      num_chunks.append(num_chunk) 
      count += 1
```

5.  提取文档单词矩阵，有效统计文档中每个单词的发生率：

```py
  from sklearn.feature_extraction.text      
  import CountVectorizer
```

6.  提取文档术语`matrix:`

```py
from sklearn.feature_extraction.text import CountVectorizer 
vectorizer = CountVectorizer(min_df=5, max_df=.95) 
matrix = vectorizer.fit_transform([num_chunk['text'] for num_chunk in num_chunks]) 
```

7.  提取词汇表并打印：

```py
vocabulary = np.array(vectorizer.get_feature_names()) 
print "nVocabulary:" 
print vocabulary 
```

8.  打印文档术语`matrix`：

```py
print "nDocument term matrix:" 
chunks_name = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4'] 
formatted_row = '{:>12}' * (len(chunks_name) + 1) 
print 'n', formatted_row.format('Word', *chunks_name), 'n' 
```

9.  在整个单词中迭代，并在不同的块中打印每个单词的再现：

```py
for word, item in zip(vocabulary, matrix.T): 
# 'item' is a 'csr_matrix' data structure 
 result = [str(x) for x in item.data] 
 print formatted_row.format(word, *result)
```

10.  执行单词包模型后得到的结果如下所示：

![](Images/be04e751-2b4b-4148-a279-59415d35db0a.png)

![](Images/c0088dd1-48ad-4485-b29b-14b920f4998e.png)

为了了解它在给定句子中的作用，请参考以下内容：

*   *情绪分析导论*，此处解释：[https://blog.algorithmia.com/introduction-sentiment-analysis/](https://blog.algorithmia.com/introduction-sentiment-analysis/)

# 文本分类器的应用

文本分类器用于分析客户情绪，在产品评论中，在互联网上搜索查询时，在社交标签中，预测研究文章的新颖性，等等。